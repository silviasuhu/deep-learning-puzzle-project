{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836b4f19",
   "metadata": {},
   "source": [
    "# Inference and visualization for DiffAssemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ef19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms as mpl_transforms\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "\n",
    "from PIL import Image, ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22829c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.full_models import *\n",
    "from src.full_dataset import *\n",
    "from src.gnn_diffusion import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531aaec",
   "metadata": {},
   "source": [
    "### 0.- Define functions for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6be4ef",
   "metadata": {},
   "source": [
    "#### 0.1.- Comparative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap helper to add PBC\n",
    "def wrap_xy(xy, low=-1.0, high=1.0):\n",
    "    span = high - low\n",
    "    return (xy - low).remainder(span) + low\n",
    "\n",
    "# Visualize one denoising step in pose space: [x, y, rot_x, rot_y]\n",
    "# (consistent with Puzzle_Dataset_ROT + GNN_Diffusion training objective)\n",
    "def _plot_pose(ax, x_pose, title, cmap_name=\"tab10\"):\n",
    "    xy = x_pose[:, :2].detach().cpu()\n",
    "    rot = x_pose[:, 2:4].detach().cpu()\n",
    "    rot = torch.nn.functional.normalize(rot, dim=1, eps=1e-8)  # stable arrows\n",
    "\n",
    "    n = xy.shape[0]\n",
    "    cmap = plt.get_cmap(cmap_name, n)  # one distinct color per patch\n",
    "    colors = cmap(np.arange(n))\n",
    "\n",
    "    ax.scatter(xy[:, 0], xy[:, 1], s=18, alpha=0.9, c=colors)\n",
    "    ax.quiver(\n",
    "        xy[:, 0], xy[:, 1],\n",
    "        rot[:, 0], rot[:, 1],\n",
    "        color=colors,\n",
    "        angles=\"xy\", scale_units=\"xy\", scale=6, width=0.004, alpha=0.8\n",
    "    )\n",
    "    ax.set_xlim(-1.15, 1.15)\n",
    "    ax.set_ylim(1.15, -1.15)  # image-like orientation\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pose_exact(ax, patches, x_pose, title, half_w, half_h, unrotate_input_k=None):\n",
    "    \"\"\"\n",
    "    patches: [N,C,H,W] tensor in [0,1]\n",
    "    x_pose:  [N,4] -> (x,y,rx,ry), with x,y normalized coordinates\n",
    "    unrotate_input_k: optional [N] int tensor (e.g. sample.rot_index) to undo dataset-applied discrete rotation first\n",
    "    \"\"\"\n",
    "    xy = x_pose[:, :2].detach().cpu()\n",
    "    rot = x_pose[:, 2:4].detach().cpu()\n",
    "    patches = patches.detach().cpu().clamp(0, 1)\n",
    "\n",
    "    if unrotate_input_k is not None:\n",
    "        k = unrotate_input_k.detach().cpu().long()\n",
    "        patches = torch.stack([torch.rot90(p, k=int((-kk.item()) % 4), dims=(1, 2)) for p, kk in zip(patches, k)])\n",
    "\n",
    "    # exact continuous angle from (rx, ry)\n",
    "    theta = torch.atan2(rot[:, 1], rot[:, 0]).numpy()  # radians\n",
    "\n",
    "    valid = torch.isfinite(xy).all(dim=1) & torch.isfinite(torch.tensor(theta))\n",
    "    xy = xy[valid]\n",
    "    theta = theta[valid.numpy() if hasattr(valid, \"numpy\") else valid]\n",
    "    patches = patches[valid]\n",
    "\n",
    "    for i in range(patches.shape[0]):\n",
    "        img = patches[i].permute(1, 2, 0).numpy()\n",
    "        x, y = float(xy[i, 0]), float(xy[i, 1])\n",
    "\n",
    "        im = ax.imshow(\n",
    "            img,\n",
    "            extent=[x - half_w, x + half_w, y - half_h, y + half_h],\n",
    "            interpolation=\"nearest\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "        im.set_transform(mpl_transforms.Affine2D().rotate_around(x, y, float(theta[i])) + ax.transData)\n",
    "\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(1.2, -1.2)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(title, color=\"white\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def show_triptych_exact(patches, x_start, x_noisy, x0_hat, t=None, unrotate_input_k=None):\n",
    "    # use clean grid spacing only for visual patch size\n",
    "    xy0 = x_start[:, :2].detach().cpu()\n",
    "    xs = torch.sort(torch.unique(xy0[:, 0]))[0]\n",
    "    ys = torch.sort(torch.unique(xy0[:, 1]))[0]\n",
    "    dx = float((xs[1] - xs[0]).item()) if len(xs) > 1 else 0.3\n",
    "    dy = float((ys[1] - ys[0]).item()) if len(ys) > 1 else 0.3\n",
    "    half_w, half_h = 0.48 * dx, 0.48 * dy\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.patch.set_facecolor(\"black\")\n",
    "\n",
    "\n",
    "    render_pose_exact(axs[0], patches, x_start, \"Clean x0\", half_w, half_h, unrotate_input_k=unrotate_input_k)\n",
    "    render_pose_exact(axs[1], patches, x_noisy, f\"Noisy xt (t={t})\" if t is not None else \"Noisy xt\", half_w, half_h, unrotate_input_k=unrotate_input_k)\n",
    "    render_pose_exact(axs[2], patches, x0_hat, \"Reconstructed x0_hat\", half_w, half_h, unrotate_input_k=unrotate_input_k)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/denoise_triptych.png\", dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "def show_denoising_exact(patches, traj, t=None, unrotate_input_k=None):\n",
    "    # use clean grid spacing only for visual patch size\n",
    "    xy0 = traj[0][:, :2].detach().cpu()\n",
    "    xs = torch.sort(torch.unique(xy0[:, 0]))[0]\n",
    "    ys = torch.sort(torch.unique(xy0[:, 1]))[0]\n",
    "    dx = float((xs[1] - xs[0]).item()) if len(xs) > 1 else 0.3\n",
    "    dy = float((ys[1] - ys[0]).item()) if len(ys) > 1 else 0.3\n",
    "    half_w, half_h = 0.48 * dx, 0.48 * dy\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(12,8))\n",
    "    fig.patch.set_facecolor(\"black\")\n",
    "    axs = axs.flatten()\n",
    "    # hide unused axes in first row\n",
    "    axs[0].axis('off')\n",
    "    axs[2].axis('off')\n",
    "    axs[4].axis('off')\n",
    "\n",
    "\n",
    "    for i in range(len(axs)):\n",
    "        if i == 0 or i == 2 or i == 4: \n",
    "            continue\n",
    "        elif i == 1:\n",
    "            render_pose_exact(axs[i], patches, traj[0], \"Clean x0 (target)\", half_w, half_h, unrotate_input_k=unrotate_input_k)\n",
    "        elif i == 3:\n",
    "            render_pose_exact(axs[i], patches, traj[1], f\"Noisy xt (t={int(t[0].item())})\", half_w, half_h, unrotate_input_k=unrotate_input_k)\n",
    "        elif i > 4:\n",
    "            render_pose_exact(axs[i], patches, traj[(i-4)*10], f\"Reverse diffusion step {(i-4)*10}\" if i < len(traj) else \"Final x0_hat\", half_w, half_h, unrotate_input_k=unrotate_input_k)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/denoise_triptych_traj.png\", dpi=600)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16804aea",
   "metadata": {},
   "source": [
    "#### 0.2.- Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_step_denoise_gif(x_start, x_noisy, x0_hat, t_value, out_path=\"outputs/denoise_timeline.gif\"):\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    frames = []\n",
    "\n",
    "    # Animation schedule\n",
    "    hold_clean = 8\n",
    "    to_noisy = 14\n",
    "    hold_noisy = 8\n",
    "    to_denoised = 14\n",
    "    hold_denoised = 10\n",
    "\n",
    "    def render_pose_to_frame(x_pose, title):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        _plot_pose(ax, x_pose, title)  # uses your existing helper from cell 15\n",
    "        fig.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        frame = np.asarray(fig.canvas.buffer_rgba())[..., :3]\n",
    "        plt.close(fig)\n",
    "        return frame\n",
    "\n",
    "    # 1) clean hold\n",
    "    for _ in range(hold_clean):\n",
    "        frames.append(render_pose_to_frame(x_start, \"Clean x0\"))\n",
    "\n",
    "    # 2) clean -> noisy\n",
    "    for a in np.linspace(0, 1, to_noisy):\n",
    "        x_mid = (1 - a) * x_start + a * x_noisy\n",
    "        frames.append(render_pose_to_frame(x_mid, f\"Noising (t={t_value})\"))\n",
    "\n",
    "    # 3) noisy hold\n",
    "    for _ in range(hold_noisy):\n",
    "        frames.append(render_pose_to_frame(x_noisy, f\"Noisy xt (t={t_value})\"))\n",
    "\n",
    "    # 4) noisy -> denoised\n",
    "    for a in np.linspace(0, 1, to_denoised):\n",
    "        x_mid = (1 - a) * x_noisy + a * x0_hat\n",
    "        frames.append(render_pose_to_frame(x_mid, \"Denoising\"))\n",
    "\n",
    "    # 5) denoised hold\n",
    "    for _ in range(hold_denoised):\n",
    "        frames.append(render_pose_to_frame(x0_hat, \"Reconstructed x0_hat\"))\n",
    "\n",
    "    imageio.mimsave(out_path, frames, fps=8, loop=0)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_step_denoise_gif_with_patches(\n",
    "    patches, x_start, x_noisy, x0_hat, t_value,\n",
    "    out_path=\"outputs/denoise_patches.gif\",\n",
    "    unrotate_input_k=None\n",
    "):\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "    # Same sizing logic as your triptych\n",
    "    xy0 = x_start[:, :2].detach().cpu()\n",
    "    xs = torch.sort(torch.unique(xy0[:, 0]))[0]\n",
    "    ys = torch.sort(torch.unique(xy0[:, 1]))[0]\n",
    "    dx = float((xs[1] - xs[0]).item()) if len(xs) > 1 else 0.3\n",
    "    dy = float((ys[1] - ys[0]).item()) if len(ys) > 1 else 0.3\n",
    "    half_w, half_h = 0.48 * dx, 0.48 * dy\n",
    "\n",
    "    frames = []\n",
    "    hold_clean, to_noisy, hold_noisy, to_denoised, hold_denoised = 8, 14, 8, 14, 10\n",
    "\n",
    "    def _render_frame(x_pose, title):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        render_pose_exact(\n",
    "            ax, patches, x_pose, title, half_w, half_h,\n",
    "            unrotate_input_k=unrotate_input_k\n",
    "        )\n",
    "        fig.patch.set_facecolor(\"black\")\n",
    "        fig.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        frame = np.asarray(fig.canvas.buffer_rgba())[..., :3]\n",
    "        plt.close(fig)\n",
    "        return frame\n",
    "\n",
    "    # 1) clean hold\n",
    "    for _ in range(hold_clean):\n",
    "        frames.append(_render_frame(x_start, \"Clean x0\"))\n",
    "\n",
    "    # 2) clean -> noisy\n",
    "    for a in np.linspace(0, 1, to_noisy):\n",
    "        x_mid = (1 - a) * x_start + a * x_noisy\n",
    "        frames.append(_render_frame(x_mid, f\"Noising (t={t_value})\"))\n",
    "\n",
    "    # 3) noisy hold\n",
    "    for _ in range(hold_noisy):\n",
    "        frames.append(_render_frame(x_noisy, f\"Noisy xt (t={t_value})\"))\n",
    "\n",
    "    # 4) noisy -> denoised\n",
    "    for a in np.linspace(0, 1, to_denoised):\n",
    "        x_mid = (1 - a) * x_noisy + a * x0_hat\n",
    "        frames.append(_render_frame(x_mid, \"Denoising\"))\n",
    "\n",
    "    # 5) denoised hold\n",
    "    for _ in range(hold_denoised):\n",
    "        frames.append(_render_frame(x0_hat, \"Reconstructed x0_hat\"))\n",
    "\n",
    "    imageio.mimsave(out_path, frames, fps=8, loop=0)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_denoise_gif_traj(\n",
    "    x_start,\n",
    "    traj,\n",
    "    t_values=None,\n",
    "    out_path=\"outputs/denoise_timeline.gif\",\n",
    "    fps=10,\n",
    "    hold_clean=8,\n",
    "    hold_final=10,\n",
    "    interp_frames=0,   # set >0 for smoother transitions between steps\n",
    "):\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    frames = []\n",
    "\n",
    "    def _to_cpu(x):\n",
    "        return x.detach().cpu() if torch.is_tensor(x) else torch.as_tensor(x)\n",
    "\n",
    "    x_start = _to_cpu(x_start)\n",
    "    traj = [_to_cpu(x) for x in traj]\n",
    "\n",
    "    if t_values is None:\n",
    "        # fallback labels: len(traj)-1 ... 0\n",
    "        t_values = list(range(len(traj) - 1, -1, -1))\n",
    "    if len(t_values) != len(traj):\n",
    "        raise ValueError(\"len(t_values) must match len(traj)\")\n",
    "\n",
    "    def render_pose_to_frame(x_pose, title):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        _plot_pose(ax, x_pose, title)\n",
    "        fig.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        frame = np.asarray(fig.canvas.buffer_rgba())[..., :3]\n",
    "        plt.close(fig)\n",
    "        return frame\n",
    "\n",
    "    for _ in range(hold_clean):\n",
    "        frames.append(render_pose_to_frame(x_start, \"Clean x0 (target)\"))\n",
    "\n",
    "    for i in range(len(traj)):\n",
    "        x_i = traj[i]\n",
    "        t_i = t_values[i]\n",
    "        frames.append(render_pose_to_frame(x_i, f\"x_t (t={t_i})\"))\n",
    "\n",
    "        if interp_frames > 0 and i < len(traj) - 1:\n",
    "            x_j = traj[i + 1]\n",
    "            t_j = t_values[i + 1]\n",
    "            for a in np.linspace(0, 1, interp_frames + 2)[1:-1]:\n",
    "                x_mid = (1 - a) * x_i + a * x_j\n",
    "                frames.append(render_pose_to_frame(x_mid, f\"Denoising {t_i}->{t_j}\"))\n",
    "\n",
    "    for _ in range(hold_final):\n",
    "        frames.append(render_pose_to_frame(traj[-1], \"Final x0_hat\"))\n",
    "\n",
    "    imageio.mimsave(out_path, frames, fps=fps, loop=0)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_denoise_gif_with_patches_traj(\n",
    "    patches,\n",
    "    x_start,\n",
    "    traj,\n",
    "    steps=None,\n",
    "    t_values=None,\n",
    "    out_path=\"outputs/denoise_patches_all_steps.gif\",\n",
    "    unrotate_input_k=None,\n",
    "    fps=10,\n",
    "    hold_x0=15,\n",
    "    hold_xt=15,\n",
    "    hold_final=15,\n",
    "    step_hold=1,\n",
    "    interp_frames=0,\n",
    "    drop_first_if_xstart=True,\n",
    "):\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "\n",
    "    x_start = x_start.detach().cpu()\n",
    "    patches = patches.detach().cpu()\n",
    "    traj_cpu = [x.detach().cpu() for x in traj]\n",
    "\n",
    "    # If traj[0] is the input pose, drop it (you already show x_start separately)\n",
    "    if (\n",
    "        drop_first_if_xstart\n",
    "        and len(traj_cpu) > 0\n",
    "        and torch.allclose(traj_cpu[0], x_start, atol=1e-6, rtol=1e-4)\n",
    "    ):\n",
    "        traj_cpu = traj_cpu[1:]\n",
    "\n",
    "    if len(traj_cpu) == 0:\n",
    "        raise ValueError(\"Trajectory is empty after preprocessing.\")\n",
    "\n",
    "    # Build timestep labels\n",
    "    if t_values is None:\n",
    "        if steps is None:\n",
    "            t_values = list(range(len(traj_cpu) - 1, -1, -1))\n",
    "        else:\n",
    "            t_values = [max(steps - 1 - i, 0) for i in range(len(traj_cpu))]\n",
    "    else:\n",
    "        # If caller passed labels for full traj and we dropped first item, align labels\n",
    "        if len(t_values) == len(traj) and len(t_values) == len(traj_cpu) + 1:\n",
    "            t_values = t_values[1:]\n",
    "        if len(t_values) != len(traj_cpu):\n",
    "            raise ValueError(\"len(t_values) must match trajectory length.\")\n",
    "\n",
    "    # Same sizing logic as your triptych/template\n",
    "    xy0 = x_start[:, :2]\n",
    "    xs = torch.sort(torch.unique(xy0[:, 0]))[0]\n",
    "    ys = torch.sort(torch.unique(xy0[:, 1]))[0]\n",
    "    dx = float((xs[1] - xs[0]).item()) if len(xs) > 1 else 0.3\n",
    "    dy = float((ys[1] - ys[0]).item()) if len(ys) > 1 else 0.3\n",
    "    half_w, half_h = 0.48 * dx, 0.48 * dy\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    def _render_frame(x_pose, title):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        render_pose_exact(\n",
    "            ax, patches, x_pose, title, half_w, half_h, unrotate_input_k=unrotate_input_k\n",
    "        )\n",
    "        fig.patch.set_facecolor(\"black\")\n",
    "        fig.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        frame = np.asarray(fig.canvas.buffer_rgba())[..., :3]\n",
    "        plt.close(fig)\n",
    "        return frame\n",
    "\n",
    "    #Render initial clean pose and hold\n",
    "    for _ in range(hold_x0):\n",
    "        frames.append(_render_frame(x_start, \"Clean x0\"))\n",
    "        \n",
    "    # Transition from x0 to xt\n",
    "    to_first_noisy = 20  # increase for smoother fade\n",
    "    first_noisy = traj_cpu[0]\n",
    "    first_t = t_values[0]\n",
    "\n",
    "    # Render transition from x_start to first noisy step\n",
    "    for a in np.linspace(0, 1, to_first_noisy + 2)[1:]:\n",
    "        x_mid = (1 - a) * x_start + a * first_noisy\n",
    "        frames.append(_render_frame(x_mid, f\"Noising (t={first_t})\"))\n",
    "    \n",
    "    # xt hold (first noisy frame)\n",
    "    for _ in range(hold_xt):\n",
    "        frames.append(_render_frame(first_noisy, f\"Noisy xt (t={first_t})\"))\n",
    "\n",
    "    # Render each step in trajectory with optional interpolation frames between steps\n",
    "    for i, (x_t, t) in enumerate(zip(traj_cpu, t_values)):\n",
    "        for _ in range(step_hold):\n",
    "            frames.append(_render_frame(x_t, f\"x_t (t={t})\"))\n",
    "\n",
    "        if interp_frames > 0 and i < len(traj_cpu) - 1:\n",
    "            x_next = traj_cpu[i + 1]\n",
    "            for a in np.linspace(0, 1, interp_frames + 2)[1:-1]:\n",
    "                x_mid = (1 - a) * x_t + a * x_next\n",
    "                frames.append(_render_frame(x_mid, f\"Denoising {t}->{t_values[i+1]}\"))\n",
    "\n",
    "    # Render final step hold\n",
    "    for _ in range(hold_final):\n",
    "        frames.append(_render_frame(traj_cpu[-1], \"Final x0_hat\"))\n",
    "\n",
    "    imageio.mimsave(out_path, frames, fps=fps, loop=0)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1c51f",
   "metadata": {},
   "source": [
    "### 0.- Define paths, steps, and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"data/CelebA-HQ\")\n",
    "print(dataset_path)\n",
    "\n",
    "steps = 100\n",
    "dims = 6\n",
    "\n",
    "model_checkpoint = \"2026_02_19_training/model_epoch90.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192833c6",
   "metadata": {},
   "source": [
    "### 1.- Load test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base dataset\n",
    "test_dataset_base = CelebA_HQ(dataset_path, train=False)\n",
    "print(f\"Test dataset length: {len(test_dataset_base)}\")\n",
    "print(f\"Sample image: \")\n",
    "plt.imshow(test_dataset_base[5812])\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420366a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load puzzle dataset and sample an element\n",
    "test_puzzle_dt = Puzzle_Dataset(\n",
    "                        dataset=test_dataset_base,\n",
    "                        #dataset_get_fn=lambda x: x,\n",
    "                        patch_per_dim=[(dims,dims)], \n",
    "                        augment=False, \n",
    "                        degree=-1, \n",
    "                        unique_graph=None)\n",
    "\n",
    "elem = test_puzzle_dt[0]\n",
    "\n",
    "print(elem)\n",
    "print(f\"X: {elem.x}\") # This contains all the node features: x, y, rot1, rot2\n",
    "print(f\"EDGE_INDEX: {elem.edge_index}\")\n",
    "print(f\"INDEXES: {elem.indexes}\")\n",
    "# print(f\"ROT: {elem.rot}\")\n",
    "# print(f\"ROT_INDEX: {elem.rot_index}\")\n",
    "print(f\"IND_NAME: {elem.ind_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7f5b3",
   "metadata": {},
   "source": [
    "### Load model with the checkpoint from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = Eff_GAT(steps=steps,\n",
    "                input_channels=4,\n",
    "                output_channels=4,\n",
    "                n_layers=4,\n",
    "                model=\"resnet18equiv\",\n",
    "                architecture=\"transformer\")\n",
    "\n",
    "# Load model with the checkpoint and set to evaluation mode\n",
    "checkpoint = torch.load(f\"checkpoints/{model_checkpoint}\",\n",
    "                        weights_only=False,\n",
    "                        map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model parameters after loading checkpoint:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbe7fe",
   "metadata": {},
   "source": [
    "### Run inference with a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c84bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 1\n",
    "test_loader = torch_geometric.loader.DataLoader(test_puzzle_dt, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True)\n",
    "\n",
    "# Get one batch\n",
    "sample = next(iter(test_loader))\n",
    "\n",
    "# Add/force \"no rotation\" feature [1, 0] for every node in the batch\n",
    "N = sample.x.size(0)  # total nodes across all graphs in batch\n",
    "rot = torch.zeros(N, 2, dtype=sample.x.dtype, device=sample.x.device)\n",
    "rot[:, 0] = 1.0\n",
    "\n",
    "if sample.x.size(1) == 2:\n",
    "    sample.x = torch.cat([sample.x, rot], dim=1)   # [N,4]\n",
    "else:\n",
    "    sample.x[:, 2:4] = rot                        # overwrite existing rot channels\n",
    "\n",
    "sample.rot = rot\n",
    "sample.rot_index = torch.zeros(N, dtype=torch.long, device=sample.x.device)\n",
    "\n",
    "print(\"x shape:\", sample.x.shape)                 # [total_nodes, 4]\n",
    "print(\"edge_index shape:\", sample.edge_index.shape)\n",
    "print(\"indexes shape:\", sample.indexes.shape)\n",
    "print(\"rot shape:\", sample.rot.shape)\n",
    "print(\"rot_index shape:\", sample.rot_index.shape)\n",
    "print(\"ind_name:\", sample.ind_name)               # one per graph\n",
    "print(\"batch vector shape:\", sample.batch.shape)  # node -> graph id\n",
    "print(\"num graphs:\", int(sample.batch.max().item()) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab987bc",
   "metadata": {},
   "source": [
    "#### Single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load criterion, inference GNN Diffusion class, and run inference step\n",
    "criterion = torch.nn.functional.smooth_l1_loss\n",
    "gnn_diffusion = GNN_Diffusion(steps=steps)\n",
    "\n",
    "if not hasattr(sample, \"batch\") or sample.batch is None:\n",
    "    sample.batch = torch.zeros(sample.x.size(0), dtype=torch.long, device=gnn_diffusion.device)\n",
    "\n",
    "# Run inference step\n",
    "with torch.no_grad():\n",
    "    num_graphs = int(sample.batch.max().item()) + 1\n",
    "    t_graph = torch.Tensor([range(gnn_diffusion.steps)[-1]], device=gnn_diffusion.device).long()\n",
    "    time = torch.gather(t_graph, 0, sample.batch)\n",
    "\n",
    "    x_start = sample.x\n",
    "    noise = torch.randn_like(x_start)\n",
    "    x_noisy = gnn_diffusion.q_sample(x_start=x_start, t=time, noise=noise)\n",
    "\n",
    "    patch_feats = model.visual_features(sample.patches)\n",
    "    pred_noise, _ = model.forward_with_feats(\n",
    "        x_noisy, time, sample.patches, sample.edge_index, patch_feats, sample.batch\n",
    "    )\n",
    "\n",
    "    # Reconstruct denoised x0 estimate from epsilon prediction\n",
    "    x0_hat = predict_x0(x_t=x_noisy, \n",
    "                        t=time, \n",
    "                        pred_noise=pred_noise, \n",
    "                        sqrt_alphas_cumprod=gnn_diffusion.sqrt_alphas_cumprod, \n",
    "                        sqrt_one_minus_alphas_cumprod=gnn_diffusion.sqrt_one_minus_alphas_cumprod\n",
    "                        )\n",
    "    \n",
    "    ### Compute metrics ###\n",
    "    # Pose\n",
    "    gt_pos, gt_rot = split_pose(x_start)\n",
    "    pred_pos, pred_rot = split_pose(x0_hat)\n",
    "\n",
    "    # Metrics\n",
    "    loss = criterion(noise, pred_noise).item()\n",
    "    pos_err = position_error(pred_pos, gt_pos)\n",
    "    rot_err = rotation_error(pred_rot, gt_rot)\n",
    "    acc_pos = piece_position_accuracy(pred_pos, gt_pos)\n",
    "    acc_rot = piece_rotation_accuracy(pred_rot, gt_rot)\n",
    "\n",
    "\n",
    "print(f\"Sample loss: {loss}\")\n",
    "print(f\"Positional error: {pos_err}\")\n",
    "print(f\"Rotational error: {rot_err}\")\n",
    "print(f\"Positional accuracy: {acc_pos}\")\n",
    "print(f\"Rotational accuracy: {acc_rot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5837093",
   "metadata": {},
   "source": [
    "####  Full step-wise inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stepwise reverse diffusion inference (100 steps)\n",
    "model.eval()\n",
    "\n",
    "# Load criterion, inference GNN Diffusion class, and run inference step\n",
    "criterion = torch.nn.functional.smooth_l1_loss\n",
    "gnn_diffusion = GNN_Diffusion(steps=steps)\n",
    "\n",
    "# Prepare sample\n",
    "sample = sample.to(gnn_diffusion.device)\n",
    "if not hasattr(sample, \"batch\") or sample.batch is None:\n",
    "    sample.batch = torch.zeros(sample.x.size(0), dtype=torch.long, device=gnn_diffusion.device)\n",
    "\n",
    "num_graphs = int(sample.batch.max().item()) + 1\n",
    "\n",
    "# same schedule as in GNN_Diffusion\n",
    "betas = torch.linspace(1e-4, 2e-2, steps, device=gnn_diffusion.device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    patch_feats = model.visual_features(sample.patches)\n",
    "\n",
    "    # start from pure noise (inference)\n",
    "    x_t = torch.randn_like(sample.x)\n",
    "\n",
    "    traj = [sample.x, x_t.detach().cpu()]  # Save intermediate states for visualization (initialize with clean x0 and noisy xt)\n",
    "    t_values = [None, steps-1]\n",
    "\n",
    "    for t_scalar in reversed(range(steps)):  # 99, 98, ..., 0\n",
    "        t_graph = torch.full((num_graphs,), t_scalar, device=gnn_diffusion.device, dtype=torch.long)\n",
    "        t = t_graph[sample.batch]  # node-level timestep\n",
    "\n",
    "        pred_noise, _ = model.forward_with_feats(\n",
    "            x_t, t, sample.patches, sample.edge_index, patch_feats, sample.batch\n",
    "        )\n",
    "\n",
    "        # Extract scalars for current timestep t\n",
    "        a_t = alphas[t].unsqueeze(-1)\n",
    "        ab_t = alphas_cumprod[t].unsqueeze(-1)\n",
    "        b_t = betas[t].unsqueeze(-1)\n",
    "\n",
    "        z = torch.randn_like(x_t) if t_scalar > 0 else torch.zeros_like(x_t)\n",
    "\n",
    "        # DDPM reverse step: x_t -> x_{t-1}\n",
    "        x_t = (1.0 / torch.sqrt(a_t)) * (\n",
    "            x_t - ((1.0 - a_t) / torch.sqrt(1.0 - ab_t + 1e-8)) * pred_noise\n",
    "        ) + torch.sqrt(b_t) * z\n",
    "\n",
    "\n",
    "        # Append values for tracking\n",
    "        traj.append(x_t.detach().cpu())\n",
    "        t_values.append(max(t_scalar-1, 0))\n",
    "\n",
    "\n",
    "x0_hat = x_t\n",
    "\n",
    "### Compute metrics ###\n",
    "# Pose\n",
    "gt_pos, gt_rot = split_pose(sample.x)\n",
    "pred_pos, pred_rot = split_pose(x0_hat)\n",
    "\n",
    "# Metrics\n",
    "pos_err = position_error(pred_pos, gt_pos)\n",
    "rot_err = rotation_error(pred_rot, gt_rot)\n",
    "acc_pos = piece_position_accuracy(pred_pos, gt_pos)\n",
    "acc_rot = piece_rotation_accuracy(pred_rot, gt_rot)\n",
    "\n",
    "\n",
    "print(\"Final metrics of the fully denoised pose:\\n\")\n",
    "print(f\"Positional error: {pos_err}\")\n",
    "print(f\"Rotational error: {rot_err}\")\n",
    "print(f\"Positional accuracy: {acc_pos}\")\n",
    "print(f\"Rotational accuracy: {acc_rot}\")\n",
    "\n",
    "\n",
    "with open(f\"outputs/metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"\"\"\\\n",
    "Final metrics of the fully denoised pose:\\n\n",
    "Positional error: {pos_err}\\n\n",
    "Rotational error: {rot_err}\\n\n",
    "Positional accuracy: {acc_pos}\\n\n",
    "Rotational accuracy: {acc_rot}\\n\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba6f6e",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84418da0",
   "metadata": {},
   "source": [
    "#### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35367e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 5, figsize=(12,8))\n",
    "axs = axs.flatten()\n",
    "# hide unused axes in first row\n",
    "axs[0].axis('off')\n",
    "axs[2].axis('off')\n",
    "axs[4].axis('off')\n",
    "\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    if i == 0 or i == 2 or i == 4: \n",
    "        continue\n",
    "    elif i == 1:\n",
    "        _plot_pose(axs[i], traj[0], \"Clean x0 (target)\")\n",
    "    elif i == 3:\n",
    "        _plot_pose(axs[i], traj[1], f\"Noisy xt (t={int(t[0].item())})\")\n",
    "    elif i > 4:\n",
    "        print((i-4)*10)\n",
    "        _plot_pose(axs[i], traj[(i-4)*10], f\"Reverse diffusion step {(i-4)*10}\" if i < len(traj) else \"Final x0_hat\")\n",
    "fig.suptitle(f\"Step-wise denoising visualization | SmoothL1(noise, pred)={loss:.5f}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/denoise_arrows_traj.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "_plot_pose(axs[0], traj[0], \"Clean x0 (target)\")\n",
    "_plot_pose(axs[1], traj[1], f\"Noisy xt (t={int(time[0].item())})\")\n",
    "_plot_pose(axs[2], traj[-1], \"Reconstructed x0_hat from predicted noise\")\n",
    "fig.suptitle(f\"Single-step denoising visualization | SmoothL1(noise, pred)={loss:.5f}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/denoise_arrows.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53ce1b",
   "metadata": {},
   "source": [
    "#### Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_triptych_exact(\n",
    "    sample.patches, traj[0], traj[1], traj[-1],\n",
    "    t=int(time[0].item()),\n",
    "    unrotate_input_k=None  # set to None if not using Puzzle_Dataset_ROT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4915dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_denoising_exact(    \n",
    "    sample.patches, traj,\n",
    "    t=time,\n",
    "    unrotate_input_k=None  # set to None if not using Puzzle_Dataset_ROT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4db2d",
   "metadata": {},
   "source": [
    "### Animation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = make_single_step_denoise_gif(\n",
    "    x_start.detach().cpu(),\n",
    "    x_noisy.detach().cpu(),\n",
    "    x0_hat.detach().cpu(),\n",
    "    t_value=int(t[0].item()),\n",
    ")\n",
    "print(\"Saved:\", gif_path)\n",
    "gif_path = make_single_step_denoise_gif_with_patches(\n",
    "    patches=sample.patches.detach().cpu(),\n",
    "    x_start=x_start.detach().cpu(),\n",
    "    x_noisy=x_noisy.detach().cpu(),\n",
    "    x0_hat=x0_hat.detach().cpu(),\n",
    "    t_value=int(t[0].item()),\n",
    "    # unrotate_input_k=sample_viz.rot_index.detach().cpu()  # optional\n",
    ")\n",
    "print(\"Saved:\", gif_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a86a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = make_denoise_gif_traj(\n",
    "    x_start=x_start.detach().cpu(),\n",
    "    traj=traj,\n",
    "    t_values=t_values,\n",
    "    out_path=\"outputs/denoise_all_steps.gif\",\n",
    "    fps=8,\n",
    "    interp_frames=0,\n",
    ")\n",
    "\n",
    "gif_path = make_denoise_gif_with_patches_traj(\n",
    "    patches=sample.patches,\n",
    "    x_start=x_start,\n",
    "    traj=traj,            # all intermediate states\n",
    "    steps=steps,          # e.g. 100\n",
    "    out_path=\"outputs/denoise_patches_all_steps.gif\",\n",
    "    unrotate_input_k=None,\n",
    ")\n",
    "print(\"Saved:\", gif_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "puzzle-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
