{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp3b_pvGKBoR"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05tty3Ys8Uh9"
   },
   "source": [
    "âš  Install the required packages only if this notebook runs in Colab. Otherwise you should install the required packages manually on your local python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5ncTcJ_-ttk"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH4Y8d75JLb1"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnYwfPkC7xb5"
   },
   "outputs": [],
   "source": [
    "# Set this to True if this notebook runs in Colab and GPU is available.\n",
    "# Ignore it if you're not in Colab\n",
    "\n",
    "COLAB_CAN_USE_GPU=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW-SxyntnIzo"
   },
   "source": [
    "Install the version 2.6.0 of torch version to be able to install later compatible pytorch-geometric packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhMOl0Z0nQ56"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip uninstall -y torch torchvision torchaudio\n",
    "  if COLAB_CAN_USE_GPU:\n",
    "    !pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
    "  else:\n",
    "    !pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQu8wvR6AAhQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if IN_COLAB:\n",
    "  torch_version = torch.__version__.split('+')[0]\n",
    "  if COLAB_CAN_USE_GPU:\n",
    "    cuda_version = torch.version.cuda.replace('.', '')\n",
    "    !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch_version}+cu{cuda_version}.html\n",
    "\n",
    "  else:\n",
    "    !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch_version}+cpu.html\n",
    "\n",
    "  !pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cezcfHMT_B5"
   },
   "source": [
    "## Inspect runtime default versions and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29RVK7PVTwRw"
   },
   "source": [
    "Check torch and torchvision default versions. For now we are just going to use them, we'll change them if we hit any conflict in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVN0jcahS4hW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(\"\")\n",
    "print(f\"Torch cuda is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1x0LeGZ5UG9E"
   },
   "source": [
    "If cuda is not available, enable GPU in Colab by going to 'Runtime' > 'Change runtime type' > Select 'T4 GPU'.\n",
    "\n",
    "This will restart the session and you'll need to rerun all the cells again. After restarting the session, verify that cuda is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt7eXwL4Uo1u"
   },
   "source": [
    "### Nvidia version\n",
    "\n",
    "The following command (nvidia-smi) will tell you which GPU you are using (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQBjDgOwSse1"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOspEcJmVMAu"
   },
   "source": [
    "## Enable cuda if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXT7ykg6Q32k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USJ53WUPVw4A"
   },
   "source": [
    "## Main imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dxqSFDPJ_aY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUirLNlN3SiQ"
   },
   "source": [
    "# Dataset download\n",
    "\n",
    "Download the dataset only if this notebook runs in Colab, otherwise you'll need to download it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO5EfN3MBFpM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IN_COLAB\"] = \"1\" if IN_COLAB else \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33HBamrvD5DS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATASET_IMG_PATH=\"data/CelebA-HQ/images\"\n",
    "    DATASET_TXT_PATH=\"data/CelebA-HQ\"\n",
    "else:\n",
    "    DATASET_IMG_PATH=\"../data/CelebA-HQ/images\"\n",
    "    DATASET_TXT_PATH=\"../data/CelebA-HQ\"\n",
    "\n",
    "os.environ[\"DATASET_IMG_PATH\"] = DATASET_IMG_PATH\n",
    "os.environ[\"DATASET_TXT_PATH\"] = DATASET_TXT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "eFV91cr13ZdH"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ \"$IN_COLAB\" == \"0\" ]; then\n",
    "  echo \"Skipping download (IN_COLAB is false)\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "echo 'Downloading dataset...'\n",
    "\n",
    "OUTPUT_FILENAME='dataset.zip'\n",
    "FOLDER_NAME='CelebAMask-HQ'\n",
    "\n",
    "mkdir -p ${DATASET_IMG_PATH}\n",
    "\n",
    "if [ -d ${DEST_FOLDER}/${FOLDER_NAME} ]; then\n",
    "  echo \"Skipping the download since the folder ${DATASET_IMG_PATH}/${FOLDER_NAME} already exists\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "rm ${OUTPUT_FILENAME}\n",
    "wget --no-check-certificate 'https://huggingface.co/datasets/liusq/CelebAMask-HQ/resolve/main/CelebAMask-HQ.zip?download=true' -O ${OUTPUT_FILENAME}\n",
    "echo \"${OUTPUT_FILENAME} downloaded. Unziping it...\"\n",
    "unzip ${OUTPUT_FILENAME}\n",
    "rm ${OUTPUT_FILENAME}\n",
    "\n",
    "mv ${FOLDER_NAME} ${DATASET_IMG_PATH}\n",
    "\n",
    "echo \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3POz1v1YCVz2"
   },
   "source": [
    "Preview an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRgNfDsoG5r5"
   },
   "outputs": [],
   "source": [
    "img = Image.open(DATASET_IMG_PATH + \"/CelebAMask-HQ/CelebA-HQ-img/1000.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VOdoBZDI8i_"
   },
   "source": [
    "Download the txt files from the DiffAssemble repository that define the data split between training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PaN4aLfJE0t"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ \"$IN_COLAB\" == \"0\" ]; then\n",
    "  echo \"Skipping download (IN_COLAB is false)\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "[ -f CelebA-HQ_test.txt ] && rm CelebA-HQ_test.txt\n",
    "[ -f CelebA-HQ_train.txt ] && rm CelebA-HQ_train.txt\n",
    "\n",
    "wget -q https://raw.githubusercontent.com/IIT-PAVIS/DiffAssemble/refs/heads/release/datasets/data_splits/CelebA-HQ_test.txt\n",
    "wget -q https://raw.githubusercontent.com/IIT-PAVIS/DiffAssemble/refs/heads/release/datasets/data_splits/CelebA-HQ_train.txt\n",
    "\n",
    "mkdir -p $DATASET_TXT_PATH\n",
    "mv CelebA-HQ_test.txt $DATASET_TXT_PATH\n",
    "mv CelebA-HQ_train.txt $DATASET_TXT_PATH\n",
    "\n",
    "ls $DATASET_TXT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_BmZ7K3IMlz"
   },
   "source": [
    "# DataSet implementation\n",
    "\n",
    "Define a basic Dataset class the solely loads images from the disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhQ4RmbbMJeS"
   },
   "outputs": [],
   "source": [
    "class CelebA_DataSet(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ONLY loads images.\n",
    "    No patches. No graphs. No diffusion logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        self.images_path = DATASET_IMG_PATH + \"/CelebAMask-HQ/CelebA-HQ-img/\"\n",
    "        if train:\n",
    "            txt_path = DATASET_TXT_PATH + \"/CelebA-HQ_train.txt\"\n",
    "        else:\n",
    "            txt_path = DATASET_TXT_PATH + \"/CelebA-HQ_test.txt\"\n",
    "\n",
    "        self.image_names = []\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.image_names = f.read().splitlines()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.images_path, self.image_names[idx]))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTlsLGmjLG1F"
   },
   "source": [
    "Test the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUji5w59LKU9"
   },
   "outputs": [],
   "source": [
    "dataset = CelebA_DataSet()\n",
    "img = dataset[0]\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRcSw0DcPBFN"
   },
   "source": [
    "Define a more complex Dataset on top of the previous one. This class will split the image in patches and return graph ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIxvsRePLwH5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from PIL.Image import Resampling\n",
    "\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def generate_random_expander(num_nodes, degree, rng=None, max_num_iters=5, exp_index=0):\n",
    "    \"\"\"Generates a random d-regular expander graph with n nodes.\n",
    "    Returns the list of edges. This list is symmetric; i.e., if\n",
    "    (x, y) is an edge so is (y,x).\n",
    "    Args:\n",
    "      num_nodes: Number of nodes in the desired graph.\n",
    "      degree: Desired degree.\n",
    "      rng: random number generator\n",
    "      max_num_iters: maximum number of iterations\n",
    "    Returns:\n",
    "      senders: tail of each edge.\n",
    "      receivers: head of each edge.\n",
    "    \"\"\"\n",
    "    if isinstance(degree, str):\n",
    "        degree = round((int(degree[:-1]) * (num_nodes - 1)) / 100)\n",
    "    num_nodes = num_nodes\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    eig_val = -1\n",
    "    eig_val_lower_bound = (\n",
    "        max(0, degree - 2 * math.sqrt(degree - 1) - 0.1) if degree > 0 else 0\n",
    "    )  # allow the use of zero degree\n",
    "\n",
    "    max_eig_val_so_far = -1\n",
    "    max_senders = []\n",
    "    max_receivers = []\n",
    "    cur_iter = 1\n",
    "\n",
    "    # (bave): This is a hack.  This should hopefully fix the bug\n",
    "    if num_nodes <= degree:\n",
    "        degree = num_nodes - 1\n",
    "\n",
    "    # (ali): if there are too few nodes, random graph generation will fail. in this case, we will\n",
    "    # add the whole graph.\n",
    "    if num_nodes <= 10:\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if i != j:\n",
    "                    max_senders.append(i)\n",
    "                    max_receivers.append(j)\n",
    "    else:\n",
    "        while eig_val < eig_val_lower_bound and cur_iter <= max_num_iters:\n",
    "            senders, receivers = generate_random_regular_graph(num_nodes, degree, rng)\n",
    "\n",
    "            eig_val = get_eigenvalue(senders, receivers, num_nodes=num_nodes)\n",
    "            if len(eig_val) == 0:\n",
    "                print(\n",
    "                    \"num_nodes = %d, degree = %d, cur_iter = %d, mmax_iters = %d, senders = %d, receivers = %d\"\n",
    "                    % (\n",
    "                        num_nodes,\n",
    "                        degree,\n",
    "                        cur_iter,\n",
    "                        max_num_iters,\n",
    "                        len(senders),\n",
    "                        len(receivers),\n",
    "                    )\n",
    "                )\n",
    "                eig_val = 0\n",
    "            else:\n",
    "                eig_val = eig_val[0]\n",
    "            if eig_val > max_eig_val_so_far:\n",
    "                max_eig_val_so_far = eig_val\n",
    "                max_senders = senders\n",
    "                max_receivers = receivers\n",
    "\n",
    "            cur_iter += 1\n",
    "    max_senders = torch.tensor(max_senders, dtype=torch.long).view(-1, 1)\n",
    "    max_receivers = torch.tensor(max_receivers, dtype=torch.long).view(-1, 1)\n",
    "    expander_edges = torch.cat([max_senders, max_receivers], dim=1)\n",
    "    return expander_edges\n",
    "\n",
    "\n",
    "def get_eigenvalue(senders, receivers, num_nodes):\n",
    "    edge_index = torch.tensor(np.stack([senders, receivers]))\n",
    "    edge_index, edge_weight = torch_geometric.utils.get_laplacian(\n",
    "        edge_index, None, normalization=None, num_nodes=num_nodes\n",
    "    )\n",
    "    L = torch_geometric.utils.to_scipy_sparse_matrix(edge_index, edge_weight, num_nodes)\n",
    "    return eigsh(L, k=2, which=\"SM\", return_eigenvectors=False)\n",
    "\n",
    "\n",
    "def generate_random_regular_graph(num_nodes, degree, rng=None):\n",
    "    \"\"\"Generates a random d-regular connected graph with n nodes.\n",
    "    Returns the list of edges. This list is symmetric; i.e., if\n",
    "    (x, y) is an edge so is (y,x).\n",
    "    Args:\n",
    "      num_nodes: Number of nodes in the desired graph.\n",
    "      degree: Desired degree.\n",
    "      rng: random number generator\n",
    "    Returns:\n",
    "      senders: tail of each edge.\n",
    "      receivers: head of each edge.\n",
    "    \"\"\"\n",
    "    if (num_nodes * degree) % 2 != 0:\n",
    "        raise TypeError(\"nodes * degree must be even\")\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    if degree == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    nodes = rng.permutation(np.arange(num_nodes))\n",
    "    num_reps = degree // 2\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    ns = np.hstack([np.roll(nodes, i + 1) for i in range(num_reps)])\n",
    "    edge_index = np.vstack((np.tile(nodes, num_reps), ns))\n",
    "\n",
    "    if degree % 2 == 0:\n",
    "        senders, receivers = np.concatenate(\n",
    "            [edge_index[0], edge_index[1]]\n",
    "        ), np.concatenate([edge_index[1], edge_index[0]])\n",
    "        return senders, receivers\n",
    "    else:\n",
    "        edge_index = np.hstack(\n",
    "            (edge_index, np.vstack((nodes[: num_nodes // 2], nodes[num_nodes // 2 :])))\n",
    "        )\n",
    "        senders, receivers = np.concatenate(\n",
    "            [edge_index[0], edge_index[1]]\n",
    "        ), np.concatenate([edge_index[1], edge_index[0]])\n",
    "        return senders, receivers\n",
    "\n",
    "class RandomCropAndResizedToOriginal(torchvision.transforms.RandomResizedCrop):\n",
    "    def forward(self, img):\n",
    "        size = img.size\n",
    "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
    "        return F.resized_crop(img, i, j, h, w, size, self.interpolation)\n",
    "\n",
    "def _get_augmentation(augmentation_type: str = \"none\"):\n",
    "    switch = {\n",
    "        \"weak\": [torchvision.transforms.RandomHorizontalFlip(p=0.5)],\n",
    "        \"hard\": [\n",
    "            torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "            RandomCropAndResizedToOriginal(\n",
    "                size=(1, 1), scale=(0.8, 1), interpolation=InterpolationMode.BICUBIC\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    "    return switch.get(augmentation_type, [])\n",
    "\n",
    "def divide_images_into_patches(\n",
    "    img, patch_per_dim: List[int], patch_size: int\n",
    ") -> List[torch.Tensor]:\n",
    "    # img2 = einops.rearrange(img, \"c h w -> h w c\")\n",
    "\n",
    "    # divide images in non-overlapping patches based on patch size\n",
    "    # output dim -> a\n",
    "    img2 = img.permute(1, 2, 0)\n",
    "    patches = img2.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size)\n",
    "    y = torch.linspace(-1, 1, patch_per_dim[0])\n",
    "    x = torch.linspace(-1, 1, patch_per_dim[1])\n",
    "    xy = torch.stack(torch.meshgrid(x, y, indexing=\"xy\"), -1)\n",
    "    # print(patch_per_dim)\n",
    "\n",
    "    return xy, patches\n",
    "\n",
    "\n",
    "# generation of a unique graph for each number of nodes\n",
    "def create_graph(patch_per_dim, degree, unique_graph):\n",
    "    # Create an empty dictionary\n",
    "    patch_edge_index_dict = {}\n",
    "    for patch_dim in patch_per_dim:\n",
    "        if degree == -1:\n",
    "            num_patches = patch_dim[0] * patch_dim[1]\n",
    "            adj_mat = torch.ones(num_patches, num_patches)\n",
    "            edge_index, _ = adj_mat.nonzero().t().contiguous()\n",
    "        else:\n",
    "            num_patches = patch_dim[0] * patch_dim[1]\n",
    "            edge_index = (\n",
    "                generate_random_expander(\n",
    "                    num_nodes=num_patches, degree=degree, rng=unique_graph\n",
    "                )\n",
    "                .t()\n",
    "                .contiguous()\n",
    "            )\n",
    "        patch_edge_index_dict[patch_dim] = edge_index\n",
    "    return patch_edge_index_dict\n",
    "\n",
    "class Puzzle_Dataset(torch_geometric.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        patch_per_dim=[(7, 6)],\n",
    "        patch_size=32,\n",
    "        augment=\"\",\n",
    "        degree=-1,\n",
    "        unique_graph=None,\n",
    "        random=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.patch_per_dim = patch_per_dim\n",
    "        self.unique_graph = unique_graph\n",
    "        self.augment = augment\n",
    "        self.random = random\n",
    "\n",
    "        self.transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                *_get_augmentation(augment),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.degree = degree\n",
    "\n",
    "        if self.unique_graph is not None:\n",
    "            self.edge_index = create_graph(\n",
    "                self.patch_per_dim, self.degree, self.unique_graph\n",
    "            )\n",
    "\n",
    "    def len(self) -> int:\n",
    "        if self.dataset is not None:\n",
    "            return len(self.dataset)\n",
    "        else:\n",
    "            raise Exception(\"Dataset not provided\")\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self.dataset is not None:\n",
    "            img = self.dataset[idx]\n",
    "\n",
    "        rdim = torch.randint(len(self.patch_per_dim), size=(1,)).item()\n",
    "        patch_per_dim = self.patch_per_dim[rdim]\n",
    "\n",
    "        height = patch_per_dim[0] * self.patch_size\n",
    "        width = patch_per_dim[1] * self.patch_size\n",
    "        img = img.resize((width, height))  # , resample=Resampling.BICUBIC)\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        xy, patches = divide_images_into_patches(img, patch_per_dim, self.patch_size)\n",
    "\n",
    "        xy = einops.rearrange(xy, \"x y c -> (x y) c\")\n",
    "\n",
    "        indexes = torch.arange(patch_per_dim[0] * patch_per_dim[1]).reshape(\n",
    "            xy.shape[:-1]\n",
    "        )\n",
    "        patches = einops.rearrange(patches, \"x y c k1 k2 -> (x y) c k1 k2\")\n",
    "        if self.random:\n",
    "            patches = patches[torch.randperm(len(patches))]\n",
    "        if self.degree == -1:\n",
    "            # all connected to all\n",
    "            adj_mat = torch.ones(\n",
    "                patch_per_dim[0] * patch_per_dim[1], patch_per_dim[0] * patch_per_dim[1]\n",
    "            )\n",
    "\n",
    "            edge_index, _ = torch_geometric.utils.dense_to_sparse(adj_mat)\n",
    "        else:\n",
    "            if not self.unique_graph:\n",
    "                edge_index = generate_random_expander(\n",
    "                    patch_per_dim[0] * patch_per_dim[1], self.degree\n",
    "                ).T\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=xy,\n",
    "            indexes=indexes,\n",
    "            patches=patches,\n",
    "            edge_index=(\n",
    "                self.edge_index[patch_per_dim] if self.unique_graph else edge_index\n",
    "            ),\n",
    "            ind_name=torch.tensor([idx]).long(),\n",
    "            patches_dim=torch.tensor([patch_per_dim]),\n",
    "        )\n",
    "        return data\n",
    "\n",
    "class Puzzle_Dataset_ROT(Puzzle_Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        patch_per_dim=[(7, 6)],\n",
    "        patch_size=32,\n",
    "        augment=False,\n",
    "        concat_rot=True,\n",
    "        degree=-1,\n",
    "        unique_graph=None,\n",
    "        all_equivariant=False,\n",
    "        random_dropout=False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            patch_per_dim=patch_per_dim,\n",
    "            patch_size=patch_size,\n",
    "            augment=augment,\n",
    "            degree=degree,\n",
    "            unique_graph=unique_graph,\n",
    "        )\n",
    "        self.concat_rot = concat_rot\n",
    "        self.degree = degree\n",
    "        self.all_equivariant = all_equivariant\n",
    "        self.unique_graph = unique_graph\n",
    "        self.random_dropout = random_dropout\n",
    "        if self.unique_graph is not None:\n",
    "            self.edge_index = create_graph(\n",
    "                self.patch_per_dim, self.degree, self.unique_graph\n",
    "            )\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self.dataset is not None:\n",
    "            img = self.dataset[idx]\n",
    "\n",
    "        rdim = torch.randint(len(self.patch_per_dim), size=(1,)).item()\n",
    "        patch_per_dim = self.patch_per_dim[rdim]\n",
    "\n",
    "        height = patch_per_dim[0] * self.patch_size\n",
    "        width = patch_per_dim[1] * self.patch_size\n",
    "\n",
    "        img = img.resize(\n",
    "            (width, height), resample=Resampling.LANCZOS\n",
    "        )  # , resample=Resampling.BICUBIC)\n",
    "\n",
    "        img = self.transforms(img)\n",
    "        xy, patches = divide_images_into_patches(img, patch_per_dim, self.patch_size)\n",
    "\n",
    "        xy = einops.rearrange(xy, \"x y c -> (x y) c\")\n",
    "        patches = einops.rearrange(patches, \"x y c k1 k2 -> (x y) c k1 k2\")\n",
    "\n",
    "        patches_num = patches.shape[0]\n",
    "\n",
    "        patches_numpy = (\n",
    "            (patches * 255).long().numpy().transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "        )\n",
    "        patches_im = [Image.fromarray(patches_numpy[x]) for x in range(patches_num)]\n",
    "        random_rot = torch.randint(low=0, high=4, size=(patches_num,))\n",
    "        random_rot_one_hot = torch.nn.functional.one_hot(random_rot, 4)\n",
    "\n",
    "        # if self.degree == '100%':\n",
    "\n",
    "        if self.degree == -1 or self.degree == \"100%\":\n",
    "            adj_mat = torch.ones(\n",
    "                patch_per_dim[0] * patch_per_dim[1], patch_per_dim[0] * patch_per_dim[1]\n",
    "            )\n",
    "\n",
    "            edge_index, _ = torch_geometric.utils.dense_to_sparse(adj_mat)\n",
    "        elif self.random_dropout:\n",
    "            adj_mat = torch.ones(\n",
    "                patch_per_dim[0] * patch_per_dim[1], patch_per_dim[0] * patch_per_dim[1]\n",
    "            )\n",
    "\n",
    "            edge_index, _ = torch_geometric.utils.dense_to_sparse(adj_mat)\n",
    "            degree = round(\n",
    "                (int(self.degree[:-1]) * (int(patch_per_dim[0] * patch_per_dim[1]) - 1))\n",
    "                / 100\n",
    "            )\n",
    "            n_connections = int(patch_per_dim[0] * patch_per_dim[1] * degree)\n",
    "            edge_index = edge_index[:, torch.randperm(edge_index.shape[1])][\n",
    "                :, :n_connections\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            if not self.unique_graph:\n",
    "                edge_index = generate_random_expander(\n",
    "                    patch_per_dim[0] * patch_per_dim[1], self.degree\n",
    "                ).T\n",
    "\n",
    "        # rotation classes : 0 -> no rotation\n",
    "        #                   1 -> 90 degrees\n",
    "        #                   2 -> 180 degrees\n",
    "        #                   3 -> 270 degrees\n",
    "\n",
    "        indexes = torch.arange(patch_per_dim[0] * patch_per_dim[1]).reshape(\n",
    "            xy.shape[:-1]\n",
    "        )\n",
    "\n",
    "        rots = torch.tensor(\n",
    "            [\n",
    "                [1, 0],\n",
    "                [0, 1],\n",
    "                [-1, 0],\n",
    "                [0, -1],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        rots_tensor = random_rot_one_hot @ rots\n",
    "\n",
    "        # ruoto l'immagine casualmente\n",
    "\n",
    "        rotated_patch = [\n",
    "            x.rotate(rot * 90) for (x, rot) in zip(patches_im, random_rot)\n",
    "        ]  # in PIL\n",
    "\n",
    "        if self.all_equivariant:\n",
    "            rotated_patch_1 = [\n",
    "                [x.rotate(rot * 90) for rot in range(4)] for x in rotated_patch\n",
    "            ]  # type: ignore\n",
    "\n",
    "            rotated_patch_tensor = [\n",
    "                [\n",
    "                    torch.tensor(np.array(patch)).permute(2, 0, 1).float() / 255\n",
    "                    for patch in test\n",
    "                ]\n",
    "                for test in rotated_patch_1\n",
    "            ]\n",
    "        else:\n",
    "            rotated_patch_tensor = [\n",
    "                torch.tensor(np.array(patch)).permute(2, 0, 1).float() / 255\n",
    "                for patch in rotated_patch\n",
    "            ]\n",
    "\n",
    "        patches = (\n",
    "            torch.stack([torch.stack(i) for i in rotated_patch_tensor])\n",
    "            if self.all_equivariant\n",
    "            else torch.stack(rotated_patch_tensor)\n",
    "        )\n",
    "        if self.concat_rot:\n",
    "            xy = torch.cat([xy, rots_tensor], 1)\n",
    "\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=xy,\n",
    "            indexes=indexes,\n",
    "            rot=rots_tensor,\n",
    "            rot_index=random_rot,\n",
    "            patches=patches,\n",
    "            edge_index=(\n",
    "                self.edge_index[patch_per_dim] if self.unique_graph else edge_index\n",
    "            ),\n",
    "            ind_name=torch.tensor([idx]).long(),\n",
    "            patches_dim=torch.tensor([patch_per_dim]),\n",
    "        )\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9lRvaWxV2qI"
   },
   "source": [
    "Inspect the output of Puzzle_Dataset_ROT\n",
    "\n",
    "###Interesting points\n",
    "\n",
    "- **Number of patches**. We'll see the image has been splited accordingly with the value assigned to the 'patch_per_dim' parameter. For instance, if patch_per_dim is [(6,6)], we'll see 36 patches per image.\n",
    "\n",
    "- **Rotation**.\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWQ4vUpOV6_1"
   },
   "outputs": [],
   "source": [
    "train_dt = CelebA_DataSet(train=True)\n",
    "\n",
    "puzzle_dt = Puzzle_Dataset_ROT(dataset=train_dt,patch_per_dim=[(6,6)], augment=False, degree=-1, unique_graph=None, all_equivariant=False, random_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPfkvuTutUHO"
   },
   "outputs": [],
   "source": [
    "elem=puzzle_dt[0]\n",
    "\n",
    "print(elem)\n",
    "print(f\"X: {elem.x}\")\n",
    "print(f\"EDGE_INDEX: {elem.edge_index}\")\n",
    "print(f\"INDEXES: {elem.indexes}\")\n",
    "print(f\"ROT: {elem.rot}\")\n",
    "print(f\"ROT_INDEX: {elem.rot_index}\")\n",
    "print(f\"IND_NAME: {elem.ind_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhixhSTJw2Id"
   },
   "outputs": [],
   "source": [
    "# Print original image\n",
    "idx = 0\n",
    "\n",
    "plt.imshow(puzzle_dt.dataset[idx])\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBs_pjvtyXaR"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "graph=puzzle_dt[idx]\n",
    "\n",
    "# rotIdx=3\n",
    "# patches = graph.patches[:, rotIdx]\n",
    "\n",
    "grid = make_grid(graph.patches, nrow=6, padding=2)\n",
    "\n",
    "# Convert CHW -> HWC for matplotlib\n",
    "grid = grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KezTEHoanu_1"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "graph=puzzle_dt[idx]\n",
    "\n",
    "grid = make_grid(graph.patches, nrow=6, padding=2)\n",
    "\n",
    "# Convert CHW -> HWC for matplotlib\n",
    "grid = grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtjtL5YVTdAk"
   },
   "source": [
    "# Backbone model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edgLbeJZTiNK"
   },
   "source": [
    "If we are in Colab, let's import the backbone model directly from our repository.\n",
    "The important elements of the imported code are:\n",
    "- Eff_GAT class. It's the entire nn.Module that will be used to train the model.\n",
    "- ResNet18 class. Is the inner module that Eff_GAT will use when it's instanciated with a \"resnet18equiv\" model.\n",
    "\n",
    "If we are not in Colab, we'll work with the srd/model folder of your current repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6wIpBnTUWZw"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ \"$IN_COLAB\" == \"0\" ]; then\n",
    "  echo \"Skipping download (IN_COLAB is false)\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "REPO_DIR=\"deep-learning-puzzle-project\"\n",
    "MODEL_DIR=\"model\"\n",
    "\n",
    "rm -r ${MODEL_DIR}\n",
    "\n",
    "git clone https://github.com/silviasuhu/deep-learning-puzzle-project.git\n",
    "cd ${REPO_DIR}\n",
    "git checkout b586dd709a8ce46465aa0284bd8d5eac812a8c94\n",
    "cd ..\n",
    "\n",
    "mv ${REPO_DIR}/src/model ${MODEL_DIR}\n",
    "rm -rf ${REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_kYvNnSVVuM"
   },
   "outputs": [],
   "source": [
    "# Let's tell to Colab that we may need to import python packages from the currect directory (which is '/content')\n",
    "import sys\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append('/content')\n",
    "else:\n",
    "  sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWOpX5pmPb8v"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ3Eif1Z-7Pd"
   },
   "source": [
    "Let's inspect first the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJFolnnx-5zq"
   },
   "outputs": [],
   "source": [
    "dataset = Puzzle_Dataset_ROT(dataset=train_dt, patch_per_dim=[(6,6)], augment=False, degree=-1, unique_graph=None, all_equivariant=True, random_dropout=False)\n",
    "\n",
    "BATCH_SIZE=10\n",
    "dataloader = torch_geometric.loader.DataLoader(\n",
    "  dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# Let's compare the dataset structure with the dataloader batch structure\n",
    "print(dataset[0])\n",
    "print(first_batch)\n",
    "\n",
    "# As you'll see, the first dimension of each parameter has been multiplied by the batch_size.\n",
    "\n",
    "# x contains...\n",
    "# edge_index contains...\n",
    "# indexes contains...\n",
    "# rot contains...\n",
    "# rot_index contains...\n",
    "# patches contains the image patches rotated 0,90,180 or 270 degrees\n",
    "# ind_name contains...\n",
    "# patches_dim contains the number of patches in the x and in the y axis.\n",
    "# batch contains...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "566FW8ioTOIX"
   },
   "source": [
    "Let's start defining a function that will run at every training iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbFOEtqnPflW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def extract(a , t):\n",
    "  out = a.gather(-1, t)\n",
    "  return out[:, None]\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "class GNN_Diffusion():\n",
    "  def __init__(self, steps):\n",
    "    self.steps=steps\n",
    "\n",
    "    # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "    betas = linear_beta_schedule(timesteps=steps)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "    self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "  def q_sample(self, x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        self.sqrt_one_minus_alphas_cumprod, t\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "  def training_step(self, batch, model, criterion, optimizer, steps):\n",
    "    print(\"Start training_step\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize the variables that will be used\n",
    "    batch_size = batch.batch.max().item() + 1\n",
    "\n",
    "    # t is a 1D tensor of size 'batch_size' with random integers between [0 and steps)\n",
    "    # It represents the diffusion time step for each graph in the batch\n",
    "    t = torch.randint(0, self.steps, (batch_size,), device=device).long()\n",
    "\n",
    "    # Expand t to match the number of nodes in the batch\n",
    "    t = torch.gather(t, 0, batch.batch.to(device))\n",
    "\n",
    "    # x_start contains the good positions and rotations of each patch\n",
    "    x_start = batch.x\n",
    "\n",
    "    # Get a random noise\n",
    "    noise = torch.randn_like(x_start)\n",
    "\n",
    "    # Compute x_noisy\n",
    "    print(\"Compute x_noisy\")\n",
    "    x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "    # Compute patch_feats\n",
    "    print(\"Compute patch_feats\")\n",
    "    patch_feats = model.visual_features(batch.patches)\n",
    "\n",
    "    # Compute prediction\n",
    "    print(\"Compute prediction\")\n",
    "    prediction, attentions = model.forward_with_feats(\n",
    "      x_noisy, t, batch.patches, batch.edge_index, patch_feats, batch.batch\n",
    "    )\n",
    "\n",
    "    # Compute loss\n",
    "    print(\"Compute loss\")\n",
    "    target=noise\n",
    "    loss = criterion(target,prediction)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXVUe4fHTUvL"
   },
   "outputs": [],
   "source": [
    "from model.efficient_gat import Eff_GAT\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "def train_model(batch_size,steps,epochs,patch_per_dim=[(6,6)]):\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  model = Eff_GAT(steps=steps,input_channels=4,output_channels=4,n_layers=4,model=\"resnet18equiv\")\n",
    "  model.to(device)\n",
    "\n",
    "  criterion = torch.nn.functional.smooth_l1_loss\n",
    "  optimizer = Adafactor(model.parameters())\n",
    "\n",
    "  # train_dt = CelebA_DataSet(train=True) #< This is not necessary since we set train_dt in a previuos cell\n",
    "  dataset = Puzzle_Dataset_ROT(dataset=train_dt, patch_per_dim=patch_per_dim, augment=False, degree=-1, unique_graph=None, all_equivariant=False, random_dropout=False)\n",
    "\n",
    "  dataloader = torch_geometric.loader.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    "  )\n",
    "\n",
    "  gnn_diffusion = GNN_Diffusion(steps=steps)\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch in dataloader:\n",
    "      loss = gnn_diffusion.training_step(batch, model, criterion, optimizer, steps)\n",
    "      losses.append(loss)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {np.mean(losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9OZJKWZ9ltV"
   },
   "outputs": [],
   "source": [
    "train_model(batch_size=10,steps=2,epochs=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
