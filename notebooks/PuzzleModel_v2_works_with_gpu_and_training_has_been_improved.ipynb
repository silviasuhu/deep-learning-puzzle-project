{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp3b_pvGKBoR"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05tty3Ys8Uh9"
   },
   "source": [
    "âš  Install the required packages only if this notebook runs in Colab. Otherwise you should install the required packages manually on your local python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1770755615380,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "-5ncTcJ_-ttk",
    "outputId": "c0a76540-d5c4-4890-d9ca-c569f232adfc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "print(IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1770755615443,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "gH4Y8d75JLb1",
    "outputId": "f99556ae-4ccc-4c23-b32e-bf7f0673df77"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1770755615456,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "HnYwfPkC7xb5"
   },
   "outputs": [],
   "source": [
    "# Set this to True if this notebook runs in Colab and GPU is available.\n",
    "# Ignore it if you're not in Colab\n",
    "\n",
    "COLAB_CAN_USE_GPU=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW-SxyntnIzo"
   },
   "source": [
    "Install the version 2.6.0 of torch version to be able to install later compatible pytorch-geometric packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45193,
     "status": "ok",
     "timestamp": 1770755660651,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "vhMOl0Z0nQ56",
    "outputId": "c2271588-bd51-4183-9682-c159ea54e2ed"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip uninstall -y torch torchvision torchaudio\n",
    "  if COLAB_CAN_USE_GPU:\n",
    "    !pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
    "  else:\n",
    "    !pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24185,
     "status": "ok",
     "timestamp": 1770755684843,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "hQu8wvR6AAhQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if IN_COLAB:\n",
    "  torch_version = torch.__version__.split('+')[0]\n",
    "  if COLAB_CAN_USE_GPU:\n",
    "    cuda_version = torch.version.cuda.replace('.', '')\n",
    "    !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch_version}+cu{cuda_version}.html\n",
    "\n",
    "  else:\n",
    "    !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch_version}+cpu.html\n",
    "\n",
    "  !pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cezcfHMT_B5"
   },
   "source": [
    "## Inspect runtime default versions and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29RVK7PVTwRw"
   },
   "source": [
    "Check torch and torchvision default versions. For now we are just going to use them, we'll change them if we hit any conflict in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1770755684883,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "cVN0jcahS4hW",
    "outputId": "e65620ca-2b6e-41ed-fa08-a8f0befbcc20"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(\"\")\n",
    "print(f\"Torch cuda is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1x0LeGZ5UG9E"
   },
   "source": [
    "If cuda is not available, enable GPU in Colab by going to 'Runtime' > 'Change runtime type' > Select 'T4 GPU'.\n",
    "\n",
    "This will restart the session and you'll need to rerun all the cells again. After restarting the session, verify that cuda is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt7eXwL4Uo1u"
   },
   "source": [
    "### Nvidia version\n",
    "\n",
    "The following command (nvidia-smi) will tell you which GPU you are using (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1770755685071,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "lQBjDgOwSse1",
    "outputId": "10068c65-7952-43f2-d73d-1e38c6ad8a2e"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOspEcJmVMAu"
   },
   "source": [
    "## Enable cuda if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770755685077,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "jXT7ykg6Q32k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USJ53WUPVw4A"
   },
   "source": [
    "## Main imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1770755685168,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "8dxqSFDPJ_aY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUirLNlN3SiQ"
   },
   "source": [
    "# Dataset download\n",
    "\n",
    "Download the dataset only if this notebook runs in Colab, otherwise you'll need to download it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1770755685169,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "dO5EfN3MBFpM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IN_COLAB\"] = \"1\" if IN_COLAB else \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1770755685170,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "33HBamrvD5DS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATASET_IMG_PATH=\"data/CelebA-HQ/images\"\n",
    "    DATASET_TXT_PATH=\"data/CelebA-HQ\"\n",
    "else:\n",
    "    DATASET_IMG_PATH=\"../data/CelebA-HQ/images\"\n",
    "    DATASET_TXT_PATH=\"../data/CelebA-HQ\"\n",
    "\n",
    "os.environ[\"DATASET_IMG_PATH\"] = DATASET_IMG_PATH\n",
    "os.environ[\"DATASET_TXT_PATH\"] = DATASET_TXT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1770755685178,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "eFV91cr13ZdH",
    "outputId": "0236294d-e411-40ec-c5a0-93946013fa27"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ \"$IN_COLAB\" == \"0\" ]; then\n",
    "  echo \"Skipping download (IN_COLAB is false)\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "echo 'Downloading dataset...'\n",
    "\n",
    "OUTPUT_FILENAME='dataset.zip'\n",
    "FOLDER_NAME='CelebAMask-HQ'\n",
    "\n",
    "mkdir -p ${DATASET_IMG_PATH}\n",
    "\n",
    "if [ -d ${DATASET_IMG_PATH}/${FOLDER_NAME} ]; then\n",
    "  echo \"Skipping the download since the folder ${DATASET_IMG_PATH}/${FOLDER_NAME} already exists\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "rm ${OUTPUT_FILENAME}\n",
    "rm -r ${FOLDER_NAME}\n",
    "wget --no-check-certificate 'https://huggingface.co/datasets/liusq/CelebAMask-HQ/resolve/main/CelebAMask-HQ.zip?download=true' -O ${OUTPUT_FILENAME}\n",
    "echo \"${OUTPUT_FILENAME} downloaded. Unziping it...\"\n",
    "unzip ${OUTPUT_FILENAME}\n",
    "rm ${OUTPUT_FILENAME}\n",
    "\n",
    "mv ${FOLDER_NAME} ${DATASET_IMG_PATH}\n",
    "\n",
    "echo \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3POz1v1YCVz2"
   },
   "source": [
    "Preview an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1770755685711,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "gRgNfDsoG5r5",
    "outputId": "07f32c7c-b7ce-4c17-ed7d-87e3b16e1841"
   },
   "outputs": [],
   "source": [
    "img = Image.open(DATASET_IMG_PATH + \"/CelebAMask-HQ/CelebA-HQ-img/1000.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VOdoBZDI8i_"
   },
   "source": [
    "Download the txt files from the DiffAssemble repository that define the data split between training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1770755685763,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "2PaN4aLfJE0t",
    "outputId": "2db93674-c9c4-4e79-8acb-6f1a763cb495"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ \"$IN_COLAB\" == \"0\" ]; then\n",
    "  echo \"Skipping download (IN_COLAB is false)\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "[ -f CelebA-HQ_test.txt ] && rm CelebA-HQ_test.txt\n",
    "[ -f CelebA-HQ_train.txt ] && rm CelebA-HQ_train.txt\n",
    "\n",
    "wget -q https://raw.githubusercontent.com/IIT-PAVIS/DiffAssemble/refs/heads/release/datasets/data_splits/CelebA-HQ_test.txt\n",
    "wget -q https://raw.githubusercontent.com/IIT-PAVIS/DiffAssemble/refs/heads/release/datasets/data_splits/CelebA-HQ_train.txt\n",
    "\n",
    "mkdir -p $DATASET_TXT_PATH\n",
    "mv CelebA-HQ_test.txt $DATASET_TXT_PATH\n",
    "mv CelebA-HQ_train.txt $DATASET_TXT_PATH\n",
    "\n",
    "ls $DATASET_TXT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_BmZ7K3IMlz"
   },
   "source": [
    "# DataSet implementation\n",
    "\n",
    "Define a basic Dataset class the solely loads images from the disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1770755685770,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "DhQ4RmbbMJeS"
   },
   "outputs": [],
   "source": [
    "class CelebA_DataSet(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ONLY loads images.\n",
    "    No patches. No graphs. No diffusion logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        self.images_path = DATASET_IMG_PATH + \"/CelebAMask-HQ/CelebA-HQ-img/\"\n",
    "        if train:\n",
    "            txt_path = DATASET_TXT_PATH + \"/CelebA-HQ_train.txt\"\n",
    "        else:\n",
    "            txt_path = DATASET_TXT_PATH + \"/CelebA-HQ_test.txt\"\n",
    "\n",
    "        self.image_names = []\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.image_names = f.read().splitlines()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "        #return 50\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.images_path, self.image_names[idx]))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTlsLGmjLG1F"
   },
   "source": [
    "Test the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1770755686058,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "VUji5w59LKU9",
    "outputId": "95b1160d-743b-4595-9bdc-1a26baa6a513"
   },
   "outputs": [],
   "source": [
    "dataset = CelebA_DataSet()\n",
    "img = dataset[0]\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRcSw0DcPBFN"
   },
   "source": [
    "Define a more complex Dataset on top of the previous one. This class will split the image in patches and return graph ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1770755686068,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "VIxvsRePLwH5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from PIL.Image import Resampling\n",
    "\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def generate_random_expander(num_nodes, degree, rng=None, max_num_iters=5, exp_index=0):\n",
    "    \"\"\"Generates a random d-regular expander graph with n nodes.\n",
    "    Returns the list of edges. This list is symmetric; i.e., if\n",
    "    (x, y) is an edge so is (y,x).\n",
    "    Args:\n",
    "      num_nodes: Number of nodes in the desired graph.\n",
    "      degree: Desired degree.\n",
    "      rng: random number generator\n",
    "      max_num_iters: maximum number of iterations\n",
    "    Returns:\n",
    "      senders: tail of each edge.\n",
    "      receivers: head of each edge.\n",
    "    \"\"\"\n",
    "    if isinstance(degree, str):\n",
    "        degree = round((int(degree[:-1]) * (num_nodes - 1)) / 100)\n",
    "    num_nodes = num_nodes\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    eig_val = -1\n",
    "    eig_val_lower_bound = (\n",
    "        max(0, degree - 2 * math.sqrt(degree - 1) - 0.1) if degree > 0 else 0\n",
    "    )  # allow the use of zero degree\n",
    "\n",
    "    max_eig_val_so_far = -1\n",
    "    max_senders = []\n",
    "    max_receivers = []\n",
    "    cur_iter = 1\n",
    "\n",
    "    # (bave): This is a hack.  This should hopefully fix the bug\n",
    "    if num_nodes <= degree:\n",
    "        degree = num_nodes - 1\n",
    "\n",
    "    # (ali): if there are too few nodes, random graph generation will fail. in this case, we will\n",
    "    # add the whole graph.\n",
    "    if num_nodes <= 10:\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if i != j:\n",
    "                    max_senders.append(i)\n",
    "                    max_receivers.append(j)\n",
    "    else:\n",
    "        while eig_val < eig_val_lower_bound and cur_iter <= max_num_iters:\n",
    "            senders, receivers = generate_random_regular_graph(num_nodes, degree, rng)\n",
    "\n",
    "            eig_val = get_eigenvalue(senders, receivers, num_nodes=num_nodes)\n",
    "            if len(eig_val) == 0:\n",
    "                print(\n",
    "                    \"num_nodes = %d, degree = %d, cur_iter = %d, mmax_iters = %d, senders = %d, receivers = %d\"\n",
    "                    % (\n",
    "                        num_nodes,\n",
    "                        degree,\n",
    "                        cur_iter,\n",
    "                        max_num_iters,\n",
    "                        len(senders),\n",
    "                        len(receivers),\n",
    "                    )\n",
    "                )\n",
    "                eig_val = 0\n",
    "            else:\n",
    "                eig_val = eig_val[0]\n",
    "            if eig_val > max_eig_val_so_far:\n",
    "                max_eig_val_so_far = eig_val\n",
    "                max_senders = senders\n",
    "                max_receivers = receivers\n",
    "\n",
    "            cur_iter += 1\n",
    "    max_senders = torch.tensor(max_senders, dtype=torch.long).view(-1, 1)\n",
    "    max_receivers = torch.tensor(max_receivers, dtype=torch.long).view(-1, 1)\n",
    "    expander_edges = torch.cat([max_senders, max_receivers], dim=1)\n",
    "    return expander_edges\n",
    "\n",
    "\n",
    "def get_eigenvalue(senders, receivers, num_nodes):\n",
    "    edge_index = torch.tensor(np.stack([senders, receivers]))\n",
    "    edge_index, edge_weight = torch_geometric.utils.get_laplacian(\n",
    "        edge_index, None, normalization=None, num_nodes=num_nodes\n",
    "    )\n",
    "    L = torch_geometric.utils.to_scipy_sparse_matrix(edge_index, edge_weight, num_nodes)\n",
    "    return eigsh(L, k=2, which=\"SM\", return_eigenvectors=False)\n",
    "\n",
    "\n",
    "def generate_random_regular_graph(num_nodes, degree, rng=None):\n",
    "    \"\"\"Generates a random d-regular connected graph with n nodes.\n",
    "    Returns the list of edges. This list is symmetric; i.e., if\n",
    "    (x, y) is an edge so is (y,x).\n",
    "    Args:\n",
    "      num_nodes: Number of nodes in the desired graph.\n",
    "      degree: Desired degree.\n",
    "      rng: random number generator\n",
    "    Returns:\n",
    "      senders: tail of each edge.\n",
    "      receivers: head of each edge.\n",
    "    \"\"\"\n",
    "    if (num_nodes * degree) % 2 != 0:\n",
    "        raise TypeError(\"nodes * degree must be even\")\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    if degree == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    nodes = rng.permutation(np.arange(num_nodes))\n",
    "    num_reps = degree // 2\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    ns = np.hstack([np.roll(nodes, i + 1) for i in range(num_reps)])\n",
    "    edge_index = np.vstack((np.tile(nodes, num_reps), ns))\n",
    "\n",
    "    if degree % 2 == 0:\n",
    "        senders, receivers = np.concatenate(\n",
    "            [edge_index[0], edge_index[1]]\n",
    "        ), np.concatenate([edge_index[1], edge_index[0]])\n",
    "        return senders, receivers\n",
    "    else:\n",
    "        edge_index = np.hstack(\n",
    "            (edge_index, np.vstack((nodes[: num_nodes // 2], nodes[num_nodes // 2 :])))\n",
    "        )\n",
    "        senders, receivers = np.concatenate(\n",
    "            [edge_index[0], edge_index[1]]\n",
    "        ), np.concatenate([edge_index[1], edge_index[0]])\n",
    "        return senders, receivers\n",
    "\n",
    "class RandomCropAndResizedToOriginal(torchvision.transforms.RandomResizedCrop):\n",
    "    def forward(self, img):\n",
    "        size = img.size\n",
    "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
    "        return F.resized_crop(img, i, j, h, w, size, self.interpolation)\n",
    "\n",
    "def _get_augmentation(augmentation_type: str = \"none\"):\n",
    "    switch = {\n",
    "        \"weak\": [torchvision.transforms.RandomHorizontalFlip(p=0.5)],\n",
    "        \"hard\": [\n",
    "            torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "            RandomCropAndResizedToOriginal(\n",
    "                size=(1, 1), scale=(0.8, 1), interpolation=InterpolationMode.BICUBIC\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    "    return switch.get(augmentation_type, [])\n",
    "\n",
    "def divide_images_into_patches(\n",
    "    img, patch_per_dim: List[int], patch_size: int\n",
    ") -> List[torch.Tensor]:\n",
    "    # img2 = einops.rearrange(img, \"c h w -> h w c\")\n",
    "\n",
    "    # divide images in non-overlapping patches based on patch size\n",
    "    # output dim -> a\n",
    "    img2 = img.permute(1, 2, 0)\n",
    "    patches = img2.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size)\n",
    "    y = torch.linspace(-1, 1, patch_per_dim[0])\n",
    "    x = torch.linspace(-1, 1, patch_per_dim[1])\n",
    "    xy = torch.stack(torch.meshgrid(x, y, indexing=\"xy\"), -1)\n",
    "    # print(patch_per_dim)\n",
    "\n",
    "    return xy, patches\n",
    "\n",
    "\n",
    "# generation of a unique graph for each number of nodes\n",
    "def create_graph(patch_per_dim, degree, unique_graph):\n",
    "    # Create an empty dictionary\n",
    "    patch_edge_index_dict = {}\n",
    "    for patch_dim in patch_per_dim:\n",
    "        if degree == -1:\n",
    "            num_patches = patch_dim[0] * patch_dim[1]\n",
    "            adj_mat = torch.ones(num_patches, num_patches)\n",
    "            edge_index, _ = adj_mat.nonzero().t().contiguous()\n",
    "        else:\n",
    "            num_patches = patch_dim[0] * patch_dim[1]\n",
    "            edge_index = (\n",
    "                generate_random_expander(\n",
    "                    num_nodes=num_patches, degree=degree, rng=unique_graph\n",
    "                )\n",
    "                .t()\n",
    "                .contiguous()\n",
    "            )\n",
    "        patch_edge_index_dict[patch_dim] = edge_index\n",
    "    return patch_edge_index_dict\n",
    "\n",
    "class Puzzle_Dataset(torch_geometric.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        patch_per_dim=[(7, 6)],\n",
    "        patch_size=32,\n",
    "        augment=\"\",\n",
    "        degree=-1,\n",
    "        unique_graph=None,\n",
    "        random=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.patch_per_dim = patch_per_dim\n",
    "        self.unique_graph = unique_graph\n",
    "        self.augment = augment\n",
    "        self.random = random\n",
    "\n",
    "        self.transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                *_get_augmentation(augment),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.degree = degree\n",
    "\n",
    "        if self.unique_graph is not None:\n",
    "            self.edge_index = create_graph(\n",
    "                self.patch_per_dim, self.degree, self.unique_graph\n",
    "            )\n",
    "\n",
    "    def len(self) -> int:\n",
    "        if self.dataset is not None:\n",
    "            return len(self.dataset)\n",
    "        else:\n",
    "            raise Exception(\"Dataset not provided\")\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self.dataset is not None:\n",
    "            img = self.dataset[idx]\n",
    "\n",
    "        rdim = torch.randint(len(self.patch_per_dim), size=(1,)).item()\n",
    "        patch_per_dim = self.patch_per_dim[rdim]\n",
    "\n",
    "        height = patch_per_dim[0] * self.patch_size\n",
    "        width = patch_per_dim[1] * self.patch_size\n",
    "        img = img.resize((width, height))  # , resample=Resampling.BICUBIC)\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        xy, patches = divide_images_into_patches(img, patch_per_dim, self.patch_size)\n",
    "\n",
    "        xy = einops.rearrange(xy, \"x y c -> (x y) c\")\n",
    "\n",
    "        indexes = torch.arange(patch_per_dim[0] * patch_per_dim[1]).reshape(\n",
    "            xy.shape[:-1]\n",
    "        )\n",
    "        patches = einops.rearrange(patches, \"x y c k1 k2 -> (x y) c k1 k2\")\n",
    "        if self.random:\n",
    "            patches = patches[torch.randperm(len(patches))]\n",
    "        if self.degree == -1:\n",
    "            # all connected to all\n",
    "            adj_mat = torch.ones(\n",
    "                patch_per_dim[0] * patch_per_dim[1], patch_per_dim[0] * patch_per_dim[1]\n",
    "            )\n",
    "\n",
    "            edge_index, _ = torch_geometric.utils.dense_to_sparse(adj_mat)\n",
    "        else:\n",
    "            if not self.unique_graph:\n",
    "                edge_index = generate_random_expander(\n",
    "                    patch_per_dim[0] * patch_per_dim[1], self.degree\n",
    "                ).T\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=xy,\n",
    "            indexes=indexes,\n",
    "            patches=patches,\n",
    "            edge_index=(\n",
    "                self.edge_index[patch_per_dim] if self.unique_graph else edge_index\n",
    "            ),\n",
    "            ind_name=torch.tensor([idx]).long(),\n",
    "            patches_dim=torch.tensor([patch_per_dim]),\n",
    "        )\n",
    "        return data\n",
    "\n",
    "class Puzzle_Dataset_ROT(Puzzle_Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        patch_per_dim=[(7, 6)],\n",
    "        patch_size=32,\n",
    "        augment=False,\n",
    "        concat_rot=True,\n",
    "        degree=-1,\n",
    "        unique_graph=None,\n",
    "        all_equivariant=False,\n",
    "        random_dropout=False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            patch_per_dim=patch_per_dim,\n",
    "            patch_size=patch_size,\n",
    "            augment=augment,\n",
    "            degree=degree,\n",
    "            unique_graph=unique_graph,\n",
    "        )\n",
    "        self.concat_rot = concat_rot\n",
    "        self.degree = degree\n",
    "        self.all_equivariant = all_equivariant\n",
    "        self.unique_graph = unique_graph\n",
    "        self.random_dropout = random_dropout\n",
    "        if self.unique_graph is not None:\n",
    "            self.edge_index = create_graph(\n",
    "                self.patch_per_dim, self.degree, self.unique_graph\n",
    "            )\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self.dataset is not None:\n",
    "            img = self.dataset[idx]\n",
    "\n",
    "        rdim = torch.randint(len(self.patch_per_dim), size=(1,)).item()\n",
    "        patch_per_dim = self.patch_per_dim[rdim]\n",
    "\n",
    "        height = patch_per_dim[0] * self.patch_size\n",
    "        width = patch_per_dim[1] * self.patch_size\n",
    "\n",
    "        img = img.resize(\n",
    "            (width, height), resample=Resampling.LANCZOS\n",
    "        )  # , resample=Resampling.BICUBIC)\n",
    "\n",
    "        img = self.transforms(img)\n",
    "        xy, patches = divide_images_into_patches(img, patch_per_dim, self.patch_size)\n",
    "\n",
    "        xy = einops.rearrange(xy, \"x y c -> (x y) c\")\n",
    "        patches = einops.rearrange(patches, \"x y c k1 k2 -> (x y) c k1 k2\")\n",
    "\n",
    "        patches_num = patches.shape[0]\n",
    "\n",
    "        patches_numpy = (\n",
    "            (patches * 255).long().numpy().transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "        )\n",
    "        patches_im = [Image.fromarray(patches_numpy[x]) for x in range(patches_num)]\n",
    "        random_rot = torch.randint(low=0, high=4, size=(patches_num,))\n",
    "        random_rot_one_hot = torch.nn.functional.one_hot(random_rot, 4)\n",
    "\n",
    "        # if self.degree == '100%':\n",
    "\n",
    "        if self.degree == -1 or self.degree == \"100%\":\n",
    "            adj_mat = torch.ones(\n",
    "                patch_per_dim[0] * patch_per_dim[1], patch_per_dim[0] * patch_per_dim[1]\n",
    "            )\n",
    "\n",
    "            edge_index, _ = torch_geometric.utils.dense_to_sparse(adj_mat)\n",
    "        elif self.random_dropout:\n",
    "            adj_mat = torch.ones(\n",
    "                patch_per_dim[0] * patch_per_dim[1], patch_per_dim[0] * patch_per_dim[1]\n",
    "            )\n",
    "\n",
    "            edge_index, _ = torch_geometric.utils.dense_to_sparse(adj_mat)\n",
    "            degree = round(\n",
    "                (int(self.degree[:-1]) * (int(patch_per_dim[0] * patch_per_dim[1]) - 1))\n",
    "                / 100\n",
    "            )\n",
    "            n_connections = int(patch_per_dim[0] * patch_per_dim[1] * degree)\n",
    "            edge_index = edge_index[:, torch.randperm(edge_index.shape[1])][\n",
    "                :, :n_connections\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            if not self.unique_graph:\n",
    "                edge_index = generate_random_expander(\n",
    "                    patch_per_dim[0] * patch_per_dim[1], self.degree\n",
    "                ).T\n",
    "\n",
    "        # rotation classes : 0 -> no rotation\n",
    "        #                   1 -> 90 degrees\n",
    "        #                   2 -> 180 degrees\n",
    "        #                   3 -> 270 degrees\n",
    "\n",
    "        indexes = torch.arange(patch_per_dim[0] * patch_per_dim[1]).reshape(\n",
    "            xy.shape[:-1]\n",
    "        )\n",
    "\n",
    "        rots = torch.tensor(\n",
    "            [\n",
    "                [1, 0],\n",
    "                [0, 1],\n",
    "                [-1, 0],\n",
    "                [0, -1],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        rots_tensor = random_rot_one_hot @ rots\n",
    "\n",
    "        # ruoto l'immagine casualmente\n",
    "\n",
    "        rotated_patch = [\n",
    "            x.rotate(rot * 90) for (x, rot) in zip(patches_im, random_rot)\n",
    "        ]  # in PIL\n",
    "\n",
    "        if self.all_equivariant:\n",
    "            rotated_patch_1 = [\n",
    "                [x.rotate(rot * 90) for rot in range(4)] for x in rotated_patch\n",
    "            ]  # type: ignore\n",
    "\n",
    "            rotated_patch_tensor = [\n",
    "                [\n",
    "                    torch.tensor(np.array(patch)).permute(2, 0, 1).float() / 255\n",
    "                    for patch in test\n",
    "                ]\n",
    "                for test in rotated_patch_1\n",
    "            ]\n",
    "        else:\n",
    "            rotated_patch_tensor = [\n",
    "                torch.tensor(np.array(patch)).permute(2, 0, 1).float() / 255\n",
    "                for patch in rotated_patch\n",
    "            ]\n",
    "\n",
    "        patches = (\n",
    "            torch.stack([torch.stack(i) for i in rotated_patch_tensor])\n",
    "            if self.all_equivariant\n",
    "            else torch.stack(rotated_patch_tensor)\n",
    "        )\n",
    "        if self.concat_rot:\n",
    "            xy = torch.cat([xy, rots_tensor], 1)\n",
    "\n",
    "        data = torch_geometric.data.Data(\n",
    "            x=xy,\n",
    "            indexes=indexes,\n",
    "            rot=rots_tensor,\n",
    "            rot_index=random_rot,\n",
    "            patches=patches,\n",
    "            edge_index=(\n",
    "                self.edge_index[patch_per_dim] if self.unique_graph else edge_index\n",
    "            ),\n",
    "            ind_name=torch.tensor([idx]).long(),\n",
    "            patches_dim=torch.tensor([patch_per_dim]),\n",
    "        )\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9lRvaWxV2qI"
   },
   "source": [
    "Inspect the output of Puzzle_Dataset_ROT\n",
    "\n",
    "###Interesting points\n",
    "\n",
    "- **Number of patches**. We'll see the image has been splited accordingly with the value assigned to the 'patch_per_dim' parameter. For instance, if patch_per_dim is [(6,6)], we'll see 36 patches per image.\n",
    "\n",
    "- **Rotation**.\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770755686073,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "tWQ4vUpOV6_1"
   },
   "outputs": [],
   "source": [
    "train_dt = CelebA_DataSet(train=True)\n",
    "\n",
    "puzzle_dt = Puzzle_Dataset_ROT(dataset=train_dt,patch_per_dim=[(6,6)], augment=False, degree=-1, unique_graph=None, all_equivariant=False, random_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1770755686121,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "JPfkvuTutUHO",
    "outputId": "9ecdf0d9-deaf-43ad-cddc-0b5bb364e095"
   },
   "outputs": [],
   "source": [
    "elem=puzzle_dt[0]\n",
    "\n",
    "print(elem)\n",
    "print(f\"X: {elem.x}\")\n",
    "print(f\"EDGE_INDEX: {elem.edge_index}\")\n",
    "print(f\"INDEXES: {elem.indexes}\")\n",
    "print(f\"ROT: {elem.rot}\")\n",
    "print(f\"ROT_INDEX: {elem.rot_index}\")\n",
    "print(f\"IND_NAME: {elem.ind_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1770755686480,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "rhixhSTJw2Id",
    "outputId": "6cfa250f-83b5-4084-e79b-fefa40ac502f"
   },
   "outputs": [],
   "source": [
    "# Print original image\n",
    "idx = 0\n",
    "\n",
    "plt.imshow(puzzle_dt.dataset[idx])\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1770755686594,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "CBs_pjvtyXaR",
    "outputId": "b0b9f9cd-e05a-453f-e3f1-194b859b47c4"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "graph=puzzle_dt[idx]\n",
    "\n",
    "# rotIdx=3\n",
    "# patches = graph.patches[:, rotIdx]\n",
    "\n",
    "grid = make_grid(graph.patches, nrow=6, padding=2)\n",
    "\n",
    "# Convert CHW -> HWC for matplotlib\n",
    "grid = grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1770755686780,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "KezTEHoanu_1",
    "outputId": "3144a79c-709a-4ec0-ca55-bce369e5a078"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "graph=puzzle_dt[idx]\n",
    "\n",
    "grid = make_grid(graph.patches, nrow=6, padding=2)\n",
    "\n",
    "# Convert CHW -> HWC for matplotlib\n",
    "grid = grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtjtL5YVTdAk"
   },
   "source": [
    "# Backbone model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edgLbeJZTiNK"
   },
   "source": [
    "If we are in Colab, let's import the backbone model directly from our repository.\n",
    "The important elements of the imported code are:\n",
    "- Eff_GAT class. It's the entire nn.Module that will be used to train the model.\n",
    "- ResNet18 class. Is the inner module that Eff_GAT will use when it's instanciated with a \"resnet18equiv\" model.\n",
    "\n",
    "If we are not in Colab, we'll work with the srd/model folder of your current repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1770755687527,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "h6wIpBnTUWZw",
    "outputId": "63100b95-924e-4771-df1c-01ce76b7e017"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ \"$IN_COLAB\" == \"0\" ]; then\n",
    "  echo \"Skipping download (IN_COLAB is false)\"\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "REPO_DIR=\"deep-learning-puzzle-project\"\n",
    "MODEL_DIR=\"model\"\n",
    "\n",
    "rm -r ${MODEL_DIR}\n",
    "\n",
    "git clone https://github.com/silviasuhu/deep-learning-puzzle-project.git\n",
    "cd ${REPO_DIR}\n",
    "git checkout b586dd709a8ce46465aa0284bd8d5eac812a8c94\n",
    "cd ..\n",
    "\n",
    "mv ${REPO_DIR}/src/model ${MODEL_DIR}\n",
    "rm -rf ${REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1770755687531,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "e_kYvNnSVVuM"
   },
   "outputs": [],
   "source": [
    "# Let's tell to Colab that we may need to import python packages from the currect directory (which is '/content')\n",
    "import sys\n",
    "\n",
    "if IN_COLAB:\n",
    "  sys.path.append('/content')\n",
    "else:\n",
    "  sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWOpX5pmPb8v"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ3Eif1Z-7Pd"
   },
   "source": [
    "Let's inspect first the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1770755688623,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "aJFolnnx-5zq",
    "outputId": "10804624-c97e-4a0a-ab7e-a1983cb89d73"
   },
   "outputs": [],
   "source": [
    "dataset = Puzzle_Dataset_ROT(dataset=train_dt, patch_per_dim=[(6,6)], augment=False, degree=-1, unique_graph=None, all_equivariant=True, random_dropout=False)\n",
    "\n",
    "BATCH_SIZE=10\n",
    "dataloader = torch_geometric.loader.DataLoader(\n",
    "  dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "# Let's compare the dataset structure with the dataloader batch structure\n",
    "print(dataset[0])\n",
    "print(first_batch)\n",
    "\n",
    "# As you'll see, the first dimension of each parameter has been multiplied by the batch_size.\n",
    "\n",
    "# x contains...\n",
    "# edge_index contains...\n",
    "# indexes contains...\n",
    "# rot contains...\n",
    "# rot_index contains...\n",
    "# patches contains the image patches rotated 0,90,180 or 270 degrees\n",
    "# ind_name contains...\n",
    "# patches_dim contains the number of patches in the x and in the y axis.\n",
    "# batch contains...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "566FW8ioTOIX"
   },
   "source": [
    "Let's start defining a function that will run at every training iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1770755688682,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "ZbFOEtqnPflW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def extract(a, t):\n",
    "    out = a.gather(-1, t)\n",
    "    return out[:, None]\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "\n",
    "class GNN_Diffusion:\n",
    "    def __init__(self, steps, device):\n",
    "        self.steps = steps\n",
    "        self.device = device\n",
    "\n",
    "        # diffusion schedule (CREATE DIRECTLY ON DEVICE)\n",
    "        betas = linear_beta_schedule(steps).to(device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(\n",
    "            1.0 - alphas_cumprod\n",
    "        )\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(\n",
    "            self.sqrt_alphas_cumprod, t\n",
    "        )\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            sqrt_alphas_cumprod_t * x_start +\n",
    "            sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, model, criterion, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch.batch.max().item() + 1\n",
    "\n",
    "        # t ON SAME DEVICE\n",
    "        t = torch.randint(\n",
    "            0, self.steps, (batch_size,),\n",
    "            device=self.device\n",
    "        ).long()\n",
    "\n",
    "        t = torch.gather(t, 0, batch.batch)\n",
    "\n",
    "        x_start = batch.x\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "\n",
    "        patch_feats = model.visual_features(batch.patches)\n",
    "\n",
    "        prediction, _ = model.forward_with_feats(\n",
    "            x_noisy,\n",
    "            t,\n",
    "            batch.patches,\n",
    "            batch.edge_index,\n",
    "            patch_feats,\n",
    "            batch.batch\n",
    "        )\n",
    "\n",
    "        loss = criterion(noise, prediction)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1770755688730,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "UXVUe4fHTUvL"
   },
   "outputs": [],
   "source": [
    "from model.efficient_gat import Eff_GAT\n",
    "from transformers.optimization import Adafactor\n",
    "from torch.utils.data import random_split\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model(batch_size,\n",
    "                steps,\n",
    "                epochs,\n",
    "                patch_per_dim=[(6, 6)],\n",
    "                use_scheduler=False):\n",
    "\n",
    "    # Start a new wandb run to track this script.\n",
    "    run = wandb.init(\n",
    "        entity=\"postgraduate-project-puzzle-upc\",\n",
    "        project=\"training-10-Feb-01\",\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"steps\": steps,\n",
    "            \"epochs\": epochs,\n",
    "            \"patch_per_dim\": patch_per_dim,\n",
    "            \"model\": \"Eff_gat\",\n",
    "            \"optimizer\": \"Adafactor\",\n",
    "            \"loss\": \"smooth_l1\",\n",
    "            \"use_scheduler\": use_scheduler,\n",
    "        })\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = Eff_GAT(steps=steps,\n",
    "                    input_channels=4,\n",
    "                    output_channels=4,\n",
    "                    n_layers=4,\n",
    "                    model=\"resnet18equiv\")\n",
    "    model.to(device)\n",
    "\n",
    "    # track gradient norms, parameters updates, detect exploding gradients early\n",
    "    wandb.watch(model, log=\"gradients\", log_freq=100)\n",
    "\n",
    "    criterion = torch.nn.functional.smooth_l1_loss\n",
    "    optimizer = Adafactor(model.parameters())\n",
    "\n",
    "    # Optional Scheduler\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        # Example: linear decay scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                    step_size=10,\n",
    "                                                    gamma=0.9)\n",
    "\n",
    "    # train_dt = CelebA_DataSet(train=True) #< This is not necessary since we set train_dt in a previuos cell\n",
    "    full_dataset = Puzzle_Dataset_ROT(dataset=train_dt,\n",
    "                                      patch_per_dim=patch_per_dim,\n",
    "                                      augment=False,\n",
    "                                      degree=-1,\n",
    "                                      unique_graph=None,\n",
    "                                      all_equivariant=False,\n",
    "                                      random_dropout=False)\n",
    "\n",
    "    # split dataset training and validation:\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(len(full_dataset) * val_ratio)\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = torch_geometric.loader.DataLoader(train_dataset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "    val_loader = torch_geometric.loader.DataLoader(val_dataset,\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "    gnn_diffusion = GNN_Diffusion(steps=steps, device=device)\n",
    "\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    latest_ckpt = checkpoint_dir / \"latest.pt\"\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if exists\n",
    "    # -------------------------\n",
    "    if latest_ckpt.exists():\n",
    "        print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
    "        checkpoint = torch.load(latest_ckpt,\n",
    "                                map_location=device,\n",
    "                                weights_only=False)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        if use_scheduler and \"scheduler_state_dict\" in checkpoint and scheduler is not None:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        print(f\"Resuming at epoch {start_epoch + 1}\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # TRAINING\n",
    "\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            loss = gnn_diffusion.training_step(batch, model, criterion,\n",
    "                                               optimizer)\n",
    "            train_losses.append(loss)\n",
    "        train_loss = np.mean(train_losses)\n",
    "\n",
    "        # VALIDATION\n",
    "\n",
    "        # switch model to evaluation mode\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        # disable gradient tracking (save memory,prevents accidental backprop)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # move batch to GPU/CPU\n",
    "                batch = batch.to(device)\n",
    "                batch_size_graphs = batch.batch.max().item() + 1\n",
    "                # One timestep per graph\n",
    "                t = torch.randint(0,\n",
    "                                  steps, (batch_size_graphs, ),\n",
    "                                  device=device).long()\n",
    "                # Expand t to node-level: each node gets its graph's timestep\n",
    "                t = torch.gather(t, 0, batch.batch)\n",
    "                # clean node feature (positions + rotations, etc.)\n",
    "                x_start = batch.x\n",
    "                noise = torch.randn_like(x_start)\n",
    "                x_noisy = gnn_diffusion.q_sample(x_start=x_start,\n",
    "                                                 t=t,\n",
    "                                                 noise=noise)\n",
    "                # CNN features from image patches\n",
    "                patch_feats = model.visual_features(batch.patches)\n",
    "                # 6. Predict noise with GNN\n",
    "                prediction, _ = model.forward_with_feats(\n",
    "                    x_noisy, t, batch.patches, batch.edge_index, patch_feats,\n",
    "                    batch.batch)\n",
    "                # Compute validation lose\n",
    "                # Target = true noise\n",
    "                # Prediction = model's noise estimate\n",
    "                val_loss = criterion(noise, prediction)\n",
    "                # Store scalar loss\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "\n",
    "            # -------- LOGGING --------\n",
    "            run.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train/loss\": train_loss,\n",
    "                \"val/loss\": val_loss\n",
    "            })\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Step scheduler\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # -------------------------\n",
    "            # Checkpoint save\n",
    "            # -------------------------\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss\n",
    "            }\n",
    "\n",
    "            if scheduler is not None:\n",
    "                checkpoint[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "\n",
    "            # Save latest checkpoint (resume)\n",
    "            torch.save(checkpoint, latest_ckpt)\n",
    "\n",
    "            # Save periodic snapshot\n",
    "            # Save every 5 epochs and the last one\n",
    "            if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
    "                snapshot_path = checkpoint_dir / f\"epoch_{epoch+1}.pt\"\n",
    "                torch.save(checkpoint, snapshot_path)\n",
    "                print(f\"Saved checkpoint snapshot: {snapshot_path}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Finish W&B\n",
    "    # -------------------------\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 5777,
     "status": "ok",
     "timestamp": 1770755694665,
     "user": {
      "displayName": "Angels C",
      "userId": "12779341440832174055"
     },
     "user_tz": -60
    },
    "id": "d9OZJKWZ9ltV",
    "outputId": "5178dd17-9f02-4334-a622-2273d73caa22"
   },
   "outputs": [],
   "source": [
    "train_model(batch_size=10,steps=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
